{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading\n",
    "\n",
    "This tutorial demonstrates how to load and visualize algorithm output data from the Aria Gen2 Pilot Dataset using the `AriaGen2PilotDataProvider`.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Load and visualize Heart Rate monitoring data\n",
    "- Access Diarization (speaker identification) results\n",
    "- Work with Hand-Object Interaction segmentation data\n",
    "- Explore Egocentric Voxel Lifting 3D scene reconstruction\n",
    "- Process Foundation Stereo depth estimation data\n",
    "- Understand data structures and API patterns for algorithm outputs\n",
    "\n",
    "## Algorithm Data Overview\n",
    "\n",
    "The Aria Gen2 Pilot Dataset includes 5 types of algorithm outputs. Please find the introduction to algorithms [here](TODO: updata link)\n",
    "\n",
    "1. **Heart Rate Monitoring**\n",
    "2. **Diarization** \n",
    "3. **Hand-Object Interaction** \n",
    "4. **Egocentric Voxel Lifting**\n",
    "5. **Foundation Stereo**\n",
    "\n",
    "**Important Notes:**\n",
    "- These are **algorithm outputs** (post-processed results), distinct from raw VRS sensor data\n",
    "- Algorithm data availability varies by sequence - not all sequences contain all algorithm outputs\n",
    "- Each algorithm has its own data structure and query patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "The following libraries are required for this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# Project Aria Tools imports\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.core.calibration import DeviceCalibration\n",
    "from projectaria_tools.utils.rerun_helpers import (\n",
    "    create_hand_skeleton_from_landmarks,\n",
    "    AriaGlassesOutline,\n",
    "    ToTransform3D\n",
    ")\n",
    "\n",
    "# Aria Gen2 Pilot Dataset imports\n",
    "from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\n",
    "from aria_gen2_pilot_dataset.data_provider.aria_gen2_pilot_dataset_data_types import (\n",
    "    HeartRateData,\n",
    "    DiarizationData,\n",
    "    HandObjectInteractionData,\n",
    "    BoundingBox3D,\n",
    "    BoundingBox2D,\n",
    "    CameraIntrinsicsAndPose\n",
    ")\n",
    "\n",
    "# Visualization library\n",
    "import rerun as rr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Provider\n",
    "\n",
    "The `AriaGen2PilotDataProvider` is the main interface for accessing data from the Aria Gen2 Pilot Dataset. It provides methods to query algorithm data, check availability, and access device calibration information.\n",
    "\n",
    "**⚠️ Important:** Update the `sequence_path` below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual path to your downloaded sequence folder\n",
    "sequence_path = \"path/to/your/sequence_folder\"\n",
    "\n",
    "# Initialize the data provider\n",
    "pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Algorithm Data\n",
    "\n",
    "Each Aria Gen2 Pilot dataset sequence may contain different algorithm outputs. Let's check what's available in this sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what algorithm data types are available in this sequence\n",
    "print(\"Algorithm Data Availability in This Sequence:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Heart Rate Monitoring:      {'✅' if pilot_data_provider.has_heart_rate_data() else '❌'}\")\n",
    "print(f\"Diarization:               {'✅' if pilot_data_provider.has_diarization_data() else '❌'}\")\n",
    "print(f\"Hand-Object Interaction:    {'✅' if pilot_data_provider.has_hand_object_interaction_data() else '❌'}\")\n",
    "print(f\"Egocentric Voxel Lifting:   {'✅' if pilot_data_provider.has_egocentric_voxel_lifting_data() else '❌'}\")\n",
    "print(f\"Foundation Stereo:         {'✅' if pilot_data_provider.has_stereo_depth_data() else '❌'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count available algorithms\n",
    "available_algorithms = [\n",
    "    pilot_data_provider.has_heart_rate_data(),\n",
    "    pilot_data_provider.has_diarization_data(),\n",
    "    pilot_data_provider.has_hand_object_interaction_data(),\n",
    "    pilot_data_provider.has_egocentric_voxel_lifting_data(),\n",
    "    pilot_data_provider.has_stereo_depth_data()\n",
    "]\n",
    "available_count = sum(available_algorithms)\n",
    "print(f\"\\nTotal available algorithms: {available_count}/5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Rate Monitoring\n",
    "\n",
    "Heart rate monitoring provides physiological data extracted from PPG (Photoplethysmography) sensors in the Aria glasses.\n",
    "\n",
    "### Heart Rate Data Structure\n",
    "\n",
    "The `HeartRateData` class contains:\n",
    "\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `timestamp_ns` | int | Timestamp in device time domain (nanoseconds) |\n",
    "| `heart_rate_bpm` | int | Heart rate in beats per minute |\n",
    "\n",
    "### Heart Rate API Reference\n",
    "\n",
    "- `has_heart_rate_data()`: Check if heart rate data is available\n",
    "- `get_heart_rate_by_index(index)`: Get heart rate data by index\n",
    "- `get_heart_rate_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)`: Get heart rate data by timestamp\n",
    "- `get_heart_rate_total_number()`: Get total number of heart rate entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Rate Data Loading and Analysis\n",
    "if pilot_data_provider.has_heart_rate_data():\n",
    "    print(\"✅ Heart Rate data is available\")\n",
    "    \n",
    "    # Get total number of heart rate entries\n",
    "    total_heart_rate = pilot_data_provider.get_heart_rate_total_number()\n",
    "    print(f\"Total heart rate entries: {total_heart_rate}\")\n",
    "    \n",
    "    # Sample first few heart rate entries\n",
    "    print(\"\\n=== Heart Rate Data Sample ===\")\n",
    "    sample_count = min(5, total_heart_rate)\n",
    "    for i in range(sample_count):\n",
    "        heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i)\n",
    "        if heart_rate_data is not None:\n",
    "            print(f\"Entry {i}: timestamp={heart_rate_data.timestamp_ns} ns, heart_rate={heart_rate_data.heart_rate_bpm} bpm\")\n",
    "    \n",
    "    # Query heart rate data by timestamp\n",
    "    if total_heart_rate > 0:\n",
    "        # Get a sample timestamp from the middle of the sequence\n",
    "        sample_heart_rate = pilot_data_provider.get_heart_rate_by_index(total_heart_rate // 2)\n",
    "        if sample_heart_rate is not None:\n",
    "            query_timestamp = sample_heart_rate.timestamp_ns\n",
    "            \n",
    "            # Query heart rate at this timestamp\n",
    "            heart_rate_at_time = pilot_data_provider.get_heart_rate_by_timestamp_ns(\n",
    "                query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if heart_rate_at_time is not None:\n",
    "                print(f\"\\nHeart rate at timestamp {query_timestamp}: {heart_rate_at_time.heart_rate_bpm} bpm\")\n",
    "else:\n",
    "    print(\"❌ Heart Rate data is not available in this sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Rate Visualization\n",
    "if pilot_data_provider.has_heart_rate_data():\n",
    "    print(\"\\n=== Visualizing Heart Rate Data ===\")\n",
    "    \n",
    "    # Initialize Rerun for visualization\n",
    "    rr.init(\"rerun_viz_heart_rate\")\n",
    "    \n",
    "    # Get all heart rate data for time series visualization\n",
    "    total_heart_rate = pilot_data_provider.get_heart_rate_total_number()\n",
    "    \n",
    "    # Sample heart rate data (every 10th entry for performance)\n",
    "    sample_indices = range(0, total_heart_rate, max(1, total_heart_rate // 50))\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i)\n",
    "        if heart_rate_data is not None:\n",
    "            # Convert timestamp to seconds for visualization\n",
    "            timestamp_seconds = heart_rate_data.timestamp_ns / 1e9\n",
    "            \n",
    "            # Set time and log heart rate as scalar (following visualizer pattern)\n",
    "            rr.set_time_seconds(\"device_time\", timestamp_seconds)\n",
    "            rr.log(\"heart_rate_bpm\", rr.Scalar(heart_rate_data.heart_rate_bpm))\n",
    "    rr.notebook_show()\n",
    "else:\n",
    "    print(\"Skipping heart rate visualization - no heart rate data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarization\n",
    "\n",
    "Diarization provides speaker identification and voice activity detection from audio data.\n",
    "\n",
    "### Diarization Data Structure\n",
    "\n",
    "The `DiarizationData` class contains:\n",
    "\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `start_timestamp_ns` | int | Start timestamp in device time domain (nanoseconds) |\n",
    "| `end_timestamp_ns` | int | End timestamp in device time domain (nanoseconds) |\n",
    "| `speaker` | str | Unique identifier of the speaker |\n",
    "| `content` | str | ASR transcription text |\n",
    "\n",
    "### Diarization API Reference\n",
    "\n",
    "- `has_diarization_data()`: Check if diarization data is available\n",
    "- `get_diarization_data_by_index(index)`: Get diarization data by index\n",
    "- `get_diarization_data_by_timestamp_ns(timestamp_ns, time_domain)`: Get diarization data containing timestamp (returns list)\n",
    "- `get_diarization_data_by_start_and_end_timestamps(start_ns, end_ns, time_domain)`: Get diarization data in time range\n",
    "- `get_diarization_data_total_number()`: Get total number of diarization entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarization Data Loading and Analysis\n",
    "if pilot_data_provider.has_diarization_data():\n",
    "    print(\"✅ Diarization data is available\")\n",
    "    \n",
    "    # Get total number of diarization entries\n",
    "    total_diarization = pilot_data_provider.get_diarization_data_total_number()\n",
    "    print(f\"Total diarization entries: {total_diarization}\")\n",
    "    \n",
    "    # Sample first few diarization entries\n",
    "    print(\"\\n=== Diarization Data Sample ===\")\n",
    "    sample_count = min(3, total_diarization)\n",
    "    for i in range(sample_count):\n",
    "        diarization_data = pilot_data_provider.get_diarization_data_by_index(i)\n",
    "        if diarization_data is not None:\n",
    "            duration_ms = (diarization_data.end_timestamp_ns - diarization_data.start_timestamp_ns) / 1e6\n",
    "            print(f\"Entry {i}:\")\n",
    "            print(f\"  Speaker: {diarization_data.speaker}\")\n",
    "            print(f\"  Duration: {duration_ms:.1f} ms\")\n",
    "            print(f\"  Content: {diarization_data.content[:100]}{'...' if len(diarization_data.content) > 100 else ''}\")\n",
    "            print()\n",
    "    \n",
    "    # Query diarization data by timestamp\n",
    "    if total_diarization > 0:\n",
    "        # Get a sample timestamp from the middle of the sequence\n",
    "        sample_diarization = pilot_data_provider.get_diarization_data_by_index(total_diarization // 2)\n",
    "        if sample_diarization is not None:\n",
    "            query_timestamp = sample_diarization.start_timestamp_ns\n",
    "            \n",
    "            # Query diarization at this timestamp\n",
    "            diarization_at_time = pilot_data_provider.get_diarization_data_by_timestamp_ns(\n",
    "                query_timestamp, TimeDomain.DEVICE_TIME\n",
    "            )\n",
    "            \n",
    "            print(f\"Diarization entries at timestamp {query_timestamp}: {len(diarization_at_time)}\")\n",
    "            for entry in diarization_at_time[:2]:  # Show first 2 entries\n",
    "                print(f\"  Speaker: {entry.speaker}, Content: {entry.content[:50]}...\")\n",
    "else:\n",
    "    print(\"❌ Diarization data is not available in this sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diarization Visualization\n",
    "if pilot_data_provider.has_diarization_data():\n",
    "    print(\"\\n=== Visualizing Diarization Data ===\")\n",
    "    \n",
    "    # Initialize Rerun for visualization\n",
    "    rr.init(\"rerun_viz_diarization\")\n",
    "    \n",
    "    # Get RGB camera stream for overlay\n",
    "    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "    \n",
    "    if rgb_stream_id is not None:\n",
    "        # Get time bounds for RGB images\n",
    "        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n",
    "        \n",
    "        # Sample a few RGB frames for visualization\n",
    "        sample_timestamps = []\n",
    "        for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n",
    "            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n",
    "        \n",
    "        # Visualize RGB images with diarization overlay\n",
    "        for timestamp_ns in sample_timestamps:\n",
    "            # Get RGB image\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if rgb_data.is_valid():\n",
    "                # Visualize the RGB image\n",
    "                rr.set_time_nanos(\"device_time\", rgb_record.capture_timestamp_ns)\n",
    "                rr.log(\"camera_rgb\", rr.Image(rgb_data.to_numpy_array()))\n",
    "                \n",
    "                # Get diarization data for this timestamp\n",
    "                diarization_entries = pilot_data_provider.get_diarization_data_by_timestamp_ns(\n",
    "                    timestamp_ns, TimeDomain.DEVICE_TIME\n",
    "                )\n",
    "                \n",
    "                # Add diarization text overlay (following visualizer pattern)\n",
    "                if diarization_entries:\n",
    "                    # Get image dimensions for positioning (following visualizer logic)\n",
    "                    width, height = rgb_data.get_width(), rgb_data.get_height()\n",
    "                    \n",
    "                    # Clear previous diarization overlays\n",
    "                    rr.log(\"camera_rgb/diarization\", rr.Clear.recursive())\n",
    "            \n",
    "                    # Plot each diarization entry (following visualizer pattern exactly)\n",
    "                    for i, conv_data in enumerate(diarization_entries[:3]):  # Show first 3 entries\n",
    "                        text_content = f\"{conv_data.speaker}: {conv_data.content}\"\n",
    "                        text_x = width // 2  # Center horizontally\n",
    "                        text_y = height - height // 15 - (i * 10 * 7)  # Bottom positioning with vertical spacing\n",
    "                        \n",
    "                        rr.log(\n",
    "                            f\"camera_rgb/diarization/conversation_text_{i}\",\n",
    "                            rr.Points2D(\n",
    "                                positions=[[text_x, text_y]],\n",
    "                                labels=[text_content],\n",
    "                                colors=[255, 255, 255],  # White text from plot_style.py DIARIZATION_TEXT\n",
    "                                radii=10  # Text size from plot_style.py\n",
    "                            )\n",
    "                        )\n",
    "    rr.notebook_show()\n",
    "else:\n",
    "    print(\"Skipping diarization visualization - no diarization data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-Object Interaction\n",
    "\n",
    "Hand-Object Interaction provides segmentation masks for hands and interacting objects, enabling analysis of hand-object relationships.\n",
    "\n",
    "### Hand-Object Interaction Data Structure\n",
    "\n",
    "The `HandObjectInteractionData` class contains:\n",
    "\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `timestamp_ns` | int | Timestamp in device time domain (nanoseconds) |\n",
    "| `category_id` | int | Category: 1=left_hand, 2=right_hand, 3=interacting_object |\n",
    "| `masks` | List[np.ndarray] | List of decoded binary masks (height, width) uint8 arrays |\n",
    "| `bboxes` | List[List[float]] | List of bounding boxes [x, y, width, height] for each mask |\n",
    "| `scores` | List[float] | List of confidence scores [0.0, 1.0] for each mask |\n",
    "\n",
    "### Hand-Object Interaction API Reference\n",
    "\n",
    "- `has_hand_object_interaction_data()`: Check if HOI data is available\n",
    "- `get_hoi_data_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)`: Get HOI data by timestamp (returns list)\n",
    "- `get_hoi_data_by_index(index)`: Get HOI data by index\n",
    "- `get_hoi_total_number()`: Get total number of HOI timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-Object Interaction Data Loading and Analysis\n",
    "if pilot_data_provider.has_hand_object_interaction_data():\n",
    "    print(\"✅ Hand-Object Interaction data is available\")\n",
    "    \n",
    "    # Get total number of HOI entries\n",
    "    total_hoi = pilot_data_provider.get_hoi_total_number()\n",
    "    print(f\"Total HOI timestamps: {total_hoi}\")\n",
    "    \n",
    "    # Sample first few HOI entries\n",
    "    print(\"\\n=== Hand-Object Interaction Data Sample ===\")\n",
    "    sample_count = min(3, total_hoi)\n",
    "    for i in range(sample_count):\n",
    "        hoi_data_list = pilot_data_provider.get_hoi_data_by_index(i)\n",
    "        if hoi_data_list is not None and len(hoi_data_list) > 0:\n",
    "            print(f\"Timestamp {i}: {len(hoi_data_list)} HOI entries\")\n",
    "            for j, hoi_data in enumerate(hoi_data_list[:2]):  # Show first 2 entries\n",
    "                category_names = {1: \"left_hand\", 2: \"right_hand\", 3: \"interacting_object\"}\n",
    "                category_name = category_names.get(hoi_data.category_id, \"unknown\")\n",
    "                print(f\"  Entry {j}: {category_name}, {len(hoi_data.masks)} masks, avg_score={np.mean(hoi_data.scores):.3f}\")\n",
    "                if len(hoi_data.masks) > 0:\n",
    "                    print(f\"    Mask shape: {hoi_data.masks[0].shape}\")\n",
    "    \n",
    "    # Query HOI data by timestamp\n",
    "    if total_hoi > 0:\n",
    "        # Get a sample timestamp from the middle of the sequence\n",
    "        sample_hoi_list = pilot_data_provider.get_hoi_data_by_index(total_hoi // 2)\n",
    "        if sample_hoi_list is not None and len(sample_hoi_list) > 0:\n",
    "            query_timestamp = sample_hoi_list[0].timestamp_ns\n",
    "            \n",
    "            # Query HOI at this timestamp\n",
    "            hoi_at_time = pilot_data_provider.get_hoi_data_by_timestamp_ns(\n",
    "                query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if hoi_at_time is not None:\n",
    "                print(f\"\\nHOI entries at timestamp {query_timestamp}: {len(hoi_at_time)}\")\n",
    "                for entry in hoi_at_time[:2]:  # Show first 2 entries\n",
    "                    category_names = {1: \"left_hand\", 2: \"right_hand\", 3: \"interacting_object\"}\n",
    "                    category_name = category_names.get(entry.category_id, \"unknown\")\n",
    "                    print(f\"  {category_name}: {len(entry.masks)} masks, scores={[f'{s:.2f}' for s in entry.scores[:3]]}\")\n",
    "else:\n",
    "    print(\"❌ Hand-Object Interaction data is not available in this sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-Object Interaction Visualization\n",
    "if pilot_data_provider.has_hand_object_interaction_data():\n",
    "    print(\"\\n=== Visualizing Hand-Object Interaction Data ===\")\n",
    "    \n",
    "    # Initialize Rerun for visualization\n",
    "    rr.init(\"rerun_viz_hoi\")\n",
    "    \n",
    "    # Get RGB camera stream for overlay\n",
    "    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "    \n",
    "    if rgb_stream_id is not None:\n",
    "        # Get time bounds for RGB images\n",
    "        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n",
    "        \n",
    "        # Sample a few RGB frames for visualization\n",
    "        sample_timestamps = []\n",
    "        for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n",
    "            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n",
    "        \n",
    "        # Visualize RGB images with HOI overlay\n",
    "        for timestamp_ns in sample_timestamps:\n",
    "            # Get RGB image\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if rgb_data.is_valid():\n",
    "                # Visualize the RGB image\n",
    "                rr.set_time_nanos(\"device_time\", rgb_record.capture_timestamp_ns)\n",
    "                rr.log(\"camera_rgb\", rr.Image(rgb_data.to_numpy_array()))\n",
    "                \n",
    "                # Get HOI data for this timestamp\n",
    "                hoi_entries = pilot_data_provider.get_hoi_data_by_timestamp_ns(\n",
    "                    timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "                )\n",
    "                \n",
    "                # Visualize HOI masks as overlays (following visualizer pattern exactly)\n",
    "                if hoi_entries:\n",
    "                    # Clear previous HOI overlays (following visualizer pattern)\n",
    "                    rr.log(\"camera_rgb/hoi_overlay\", rr.Clear.recursive())\n",
    "                    \n",
    "                    # Filter out HOI data too far away from the current frame (following visualizer logic)\n",
    "                    rgb_frame_interval_ns = 33_333_333  # ~30 FPS\n",
    "                    if abs(hoi_entries[0].timestamp_ns - timestamp_ns) > rgb_frame_interval_ns / 2:\n",
    "                        continue\n",
    "                    \n",
    "                    # Color mapping from plot_style.py (following visualizer pattern)\n",
    "                    category_to_plot_style = {\n",
    "                        1: [119, 172, 48, 128],    # Green for left hand (HOI_LEFT_HAND)\n",
    "                        2: [217, 83, 255, 128],    # Purple for right hand (HOI_RIGHT_HAND)\n",
    "                        3: [237, 177, 32, 128]     # Orange for interacting object (HOI_INTERACTING_OBJECT)\n",
    "                    }\n",
    "                    \n",
    "                    # Determine mask shape from the first valid mask (following visualizer logic)\n",
    "                    mask_shape = next(\n",
    "                        (\n",
    "                            mask.shape\n",
    "                            for hoi_data in hoi_entries\n",
    "                            for mask in hoi_data.masks\n",
    "                            if mask is not None and mask.size > 0\n",
    "                        ),\n",
    "                        None,\n",
    "                    )\n",
    "                    if mask_shape is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Initialize combined RGBA overlay (following visualizer pattern)\n",
    "                    combined_rgba_overlay = np.zeros((*mask_shape, 4), dtype=np.uint8)\n",
    "                    \n",
    "                    # Overlay each category's mask with its color (following visualizer logic)\n",
    "                    for hoi_data in hoi_entries:\n",
    "                        category_id = hoi_data.category_id\n",
    "                        plot_style_color = category_to_plot_style.get(category_id, None)\n",
    "                        if not plot_style_color:\n",
    "                            continue\n",
    "                        \n",
    "                        for mask in hoi_data.masks:\n",
    "                            if mask is None or mask.size == 0:\n",
    "                                continue\n",
    "                            foreground_pixels = mask > 0\n",
    "                            combined_rgba_overlay[foreground_pixels] = plot_style_color\n",
    "                    \n",
    "                    # Log the combined segmentation overlay as an image (following visualizer pattern)\n",
    "                    rr.log(\n",
    "                        \"camera_rgb/hoi_overlay/combined\",\n",
    "                        rr.Image(combined_rgba_overlay)\n",
    "                    )\n",
    "    rr.notebook_show()\n",
    "else:\n",
    "    print(\"Skipping HOI visualization - no HOI data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Egocentric Voxel Lifting\n",
    "\n",
    "Egocentric Voxel Lifting provides 3D scene reconstruction from egocentric view, including 3D bounding boxes and object instance information.\n",
    "\n",
    "### Egocentric Voxel Lifting Data Structure\n",
    "\n",
    "The EVL system provides two main data types:\n",
    "\n",
    "**BoundingBox3D** (3D world coordinates):\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `start_timestamp_ns` | int | Timestamp in device time domain (nanoseconds) |\n",
    "| `bbox3d` | BoundingBox3dData | 3D bounding box data (AABB, transform, etc.) |\n",
    "\n",
    "**BoundingBox3dData** structure:\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `transform_scene_object` | SE3 | Object 6DoF pose in the scene (world), where: point_in_scene = T_Scene_Object * point_in_object |\n",
    "| `aabb` | List[float] | Object AABB (axes-aligned-bounding-box) in the object's local coordinate frame, represented as [xmin, xmax, ymin, ymax, zmin, zmax] |\n",
    "\n",
    "**BoundingBox2D** (2D camera projections):\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `start_timestamp_ns` | int | Timestamp in device time domain (nanoseconds) |\n",
    "| `bbox2d` | BoundingBox2dData | 2D bounding box data |\n",
    "\n",
    "**BoundingBox2dData** structure:\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `box_range` | List[float] | 2D bounding box range as [xmin, xmax, ymin, ymax] |\n",
    "| `visibility_ratio` | float | Visibility ratio calculated by occlusion between objects. visibility_ratio = 1: object is not occluded, visibility_ratio = 0: object is fully occluded |\n",
    "\n",
    "**InstanceInfo** (object metadata):\n",
    "- `category`: Object category name\n",
    "- `name`: Specific object name\n",
    "\n",
    "### Egocentric Voxel Lifting API Reference\n",
    "\n",
    "- `has_egocentric_voxel_lifting_data()`: Check if EVL data is available\n",
    "- `get_evl_3d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)`: Get 3D bounding boxes (returns Dict[int, BoundingBox3D])\n",
    "- `get_evl_2d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, camera_label)`: Get 2D bounding boxes for specific camera\n",
    "- `get_evl_instance_info_by_id(instance_id)`: Get object category/name information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egocentric Voxel Lifting Data Loading and Analysis\n",
    "if pilot_data_provider.has_egocentric_voxel_lifting_data():\n",
    "    print(\"✅ Egocentric Voxel Lifting data is available\")\n",
    "    \n",
    "    # Get RGB camera stream for 2D projection\n",
    "    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "    \n",
    "    if rgb_stream_id is not None:\n",
    "        # Get a sample timestamp from RGB stream\n",
    "        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n",
    "        sample_timestamp = first_timestamp_ns + int(5e9)  # 5 seconds into sequence\n",
    "        \n",
    "        # Query 3D bounding boxes\n",
    "        evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns(\n",
    "            sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "        )\n",
    "        \n",
    "        if evl_3d_bboxes is not None:\n",
    "            print(f\"\\n=== EVL 3D Bounding Boxes at timestamp {sample_timestamp} ===\")\n",
    "            print(f\"Found {len(evl_3d_bboxes)} 3D bounding boxes\")\n",
    "            \n",
    "            for instance_id, bbox_3d in list(evl_3d_bboxes.items())[:3]:  # Show first 3\n",
    "                # Get instance info\n",
    "                instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id)\n",
    "                if instance_info is not None:\n",
    "                    print(f\"Instance {instance_id}: {instance_info.category} - {instance_info.name}\")\n",
    "                    print(f\"  AABB: {bbox_3d.bbox3d.aabb}\")\n",
    "                    print(f\"  Transform: {bbox_3d.bbox3d.transform_scene_object.to_matrix()[:3, 3]}\")\n",
    "        \n",
    "        # Query 2D bounding boxes for RGB camera\n",
    "        evl_2d_bboxes = pilot_data_provider.get_evl_2d_bounding_boxes_by_timestamp_ns(\n",
    "            sample_timestamp, TimeDomain.DEVICE_TIME, \"camera-rgb\"\n",
    "        )\n",
    "        \n",
    "        if evl_2d_bboxes is not None:\n",
    "            print(f\"\\n=== EVL 2D Bounding Boxes for RGB camera ===\")\n",
    "            print(f\"Found {len(evl_2d_bboxes)} 2D bounding boxes\")\n",
    "            \n",
    "            for instance_id, bbox_2d in list(evl_2d_bboxes.items())[:3]:  # Show first 3\n",
    "                print(f\"Instance {instance_id}: 2D bbox {bbox_2d.bbox2d.box_range}\")\n",
    "else:\n",
    "    print(\"❌ Egocentric Voxel Lifting data is not available in this sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aria_gen2_pilot_dataset.visualization.plot_utils import extract_bbox_projection_data, project_3d_bbox_to_2d_camera\n",
    "# Egocentric Voxel Lifting Visualization\n",
    "if pilot_data_provider.has_egocentric_voxel_lifting_data():\n",
    "    print(\"\\n=== Visualizing Egocentric Voxel Lifting Data ===\")\n",
    "    \n",
    "    # Initialize Rerun for visualization\n",
    "    rr.init(\"rerun_viz_evl\")\n",
    "    \n",
    "    # Get RGB camera stream for 2D projection\n",
    "    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "    \n",
    "    if rgb_stream_id is not None:\n",
    "        # Get time bounds for RGB images\n",
    "        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n",
    "        \n",
    "        # Sample a few RGB frames for visualization\n",
    "        sample_timestamps = []\n",
    "        for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n",
    "            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n",
    "        \n",
    "        # Visualize RGB images with EVL 2D and 3D bounding boxes\n",
    "        for timestamp_ns in sample_timestamps:\n",
    "            # Get RGB image\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if rgb_data.is_valid():\n",
    "                # Visualize the RGB image\n",
    "                rr.set_time_nanos(\"device_time\", rgb_record.capture_timestamp_ns)\n",
    "                rr.log(\"camera_rgb\", rr.Image(rgb_data.to_numpy_array()))\n",
    "                \n",
    "                # Get EVL 3D bounding boxes for this timestamp\n",
    "                evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns(\n",
    "                    timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "                )\n",
    "                \n",
    "                # Visualize projected 3D bounding boxes (following visualizer pattern)\n",
    "                if evl_3d_bboxes is not None:\n",
    "                    # Clear previous EVL overlays (following visualizer pattern)\n",
    "                    rr.log(\"camera_rgb/evl_3d_bboxes_projected\", rr.Clear.recursive())\n",
    "                    \n",
    "                    # Get trajectory pose from MPS data (following visualizer pattern exactly)\n",
    "                    trajectory_pose = pilot_data_provider.get_mps_closed_loop_pose(\n",
    "                        timestamp_ns, TimeDomain.DEVICE_TIME\n",
    "                    )\n",
    "                    \n",
    "                    if trajectory_pose is not None:\n",
    "                        # Get RGB camera calibration for projection\n",
    "                        device_calibration = pilot_data_provider.get_vrs_device_calibration()\n",
    "                        rgb_camera_calibration = device_calibration.get_camera_calib(\"camera-rgb\")\n",
    "                        \n",
    "                        if rgb_camera_calibration is not None:\n",
    "                            # Get transforms and image dimensions (following visualizer pattern exactly)\n",
    "                            T_world_device = trajectory_pose.transform_world_device\n",
    "                            T_device_camera = rgb_camera_calibration.get_transform_device_camera()\n",
    "                            T_world_camera = T_world_device @ T_device_camera\n",
    "                            \n",
    "                            # Get image dimensions\n",
    "                            image_width, image_height = rgb_camera_calibration.get_image_size()\n",
    "                            \n",
    "                            # Extract bbox data for projection using utility function (following visualizer pattern)\n",
    "                            projection_data = extract_bbox_projection_data(pilot_data_provider, evl_3d_bboxes)\n",
    "                            \n",
    "                            # Collect all projection results for batching\n",
    "                            all_projected_lines = []\n",
    "                            all_line_colors = []\n",
    "                            label_positions = []\n",
    "                            label_texts = []\n",
    "                            label_colors = []\n",
    "                            \n",
    "                            # Project each bounding box using utility function (following visualizer pattern)\n",
    "                            for data in projection_data:\n",
    "                                projection_result = project_3d_bbox_to_2d_camera(\n",
    "                                    corners_in_world=data[\"corners_world\"],\n",
    "                                    T_world_camera=T_world_camera,\n",
    "                                    camera_calibration=rgb_camera_calibration,\n",
    "                                    image_width=image_width,\n",
    "                                    image_height=image_height,\n",
    "                                    label=data[\"label\"],\n",
    "                                )\n",
    "                                \n",
    "                                if projection_result:\n",
    "                                    projected_lines, line_colors, label_position = projection_result\n",
    "                                    \n",
    "                                    # Collect projection data for batching\n",
    "                                    if projected_lines:\n",
    "                                        all_projected_lines.extend(projected_lines)\n",
    "                                        if line_colors and len(line_colors) >= len(projected_lines):\n",
    "                                            all_line_colors.extend(line_colors[:len(projected_lines)])\n",
    "                                        else:\n",
    "                                            all_line_colors.extend([0, 255, 0] * len(projected_lines))  # Green color\n",
    "                                        \n",
    "                                        if label_position and data[\"label\"]:\n",
    "                                            label_positions.append(label_position)\n",
    "                                            label_texts.append(data[\"label\"])\n",
    "                                            label_colors.append([0, 255, 0])  # Green text\n",
    "                            \n",
    "                            # Log all projected lines in batch (following visualizer pattern)\n",
    "                            if all_projected_lines:\n",
    "                                rr.log(\n",
    "                                    \"camera_rgb/evl_3d_bboxes_projected/wireframes\",\n",
    "                                    rr.LineStrips2D(\n",
    "                                        all_projected_lines,\n",
    "                                        colors=all_line_colors,\n",
    "                                        radii=1.5  # Match plot_style.py EVL line thickness\n",
    "                                    )\n",
    "                                )\n",
    "                            \n",
    "                            # Log all labels in batch (following visualizer pattern)\n",
    "                            if label_positions:\n",
    "                                rr.log(\n",
    "                                    \"camera_rgb/evl_3d_bboxes_projected/labels\",\n",
    "                                    rr.Points2D(\n",
    "                                        positions=label_positions,\n",
    "                                        labels=label_texts,\n",
    "                                        colors=label_colors,\n",
    "                                        radii=10  # Text size from plot_style.py\n",
    "                                    )\n",
    "                                )\n",
    "                \n",
    "                # Visualize 3D bounding boxes in world coordinates (following visualizer pattern exactly)\n",
    "                if evl_3d_bboxes is not None:\n",
    "                    # Clear previous 3D bounding boxes (following visualizer pattern)\n",
    "                    rr.log(\"world/evl_3d_bboxes\", rr.Clear.recursive())\n",
    "                    \n",
    "                    bb3d_sizes = []\n",
    "                    bb3d_centers = []\n",
    "                    bb3d_quats_xyzw = []\n",
    "                    bb3d_labels = []\n",
    "                    \n",
    "                    for instance_id, boundingBox3d in evl_3d_bboxes.items():\n",
    "                        # Extract BoundingBox3dData from our BoundingBox3D wrapper (following visualizer logic)\n",
    "                        bbox3d_data = boundingBox3d.bbox3d\n",
    "                        \n",
    "                        # Get AABB in object's local coordinates: [xmin, xmax, ymin, ymax, zmin, zmax]\n",
    "                        aabb = bbox3d_data.aabb\n",
    "                        \n",
    "                        # Calculate dimensions (following visualizer logic)\n",
    "                        object_dimensions = np.array([\n",
    "                            aabb[1] - aabb[0],  # width (xmax - xmin)\n",
    "                            aabb[3] - aabb[2],  # height (ymax - ymin)\n",
    "                            aabb[5] - aabb[4],  # depth (zmax - zmin)\n",
    "                        ])\n",
    "                        \n",
    "                        # Get world center and rotation from transform_scene_object (following visualizer logic)\n",
    "                        T_scene_object = bbox3d_data.transform_scene_object\n",
    "                        quat_and_translation = np.squeeze(T_scene_object.to_quat_and_translation())\n",
    "                        quaternion_wxyz = quat_and_translation[0:4]  # [w, x, y, z]\n",
    "                        world_center = quat_and_translation[4:7]  # [x, y, z]\n",
    "                        \n",
    "                        # Convert quaternion to ReRun format [x, y, z, w] (following visualizer logic)\n",
    "                        quat_xyzw = [\n",
    "                            quaternion_wxyz[1],\n",
    "                            quaternion_wxyz[2],\n",
    "                            quaternion_wxyz[3],\n",
    "                            quaternion_wxyz[0],\n",
    "                        ]\n",
    "                        \n",
    "                        # Get label (following visualizer logic)\n",
    "                        label = f\"instance_{instance_id}\"\n",
    "                        instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id)\n",
    "                        if instance_info:\n",
    "                            if hasattr(instance_info, \"category\") and instance_info.category:\n",
    "                                label = instance_info.category\n",
    "                            elif hasattr(instance_info, \"name\") and instance_info.name:\n",
    "                                label = instance_info.name\n",
    "                        \n",
    "                        # Add to lists (following visualizer pattern)\n",
    "                        bb3d_centers.append(world_center)\n",
    "                        bb3d_sizes.append(object_dimensions)\n",
    "                        bb3d_quats_xyzw.append(quat_xyzw)\n",
    "                        bb3d_labels.append(label)\n",
    "                    \n",
    "                    # Visualize using ReRun Boxes3D with plot style (following visualizer pattern exactly)\n",
    "                    if bb3d_sizes:\n",
    "                        # Split into batches of 20 (ReRun limitation, following visualizer logic)\n",
    "                        MAX_BOXES_PER_BATCH = 20\n",
    "                        batch_id = 0\n",
    "                        \n",
    "                        while batch_id * MAX_BOXES_PER_BATCH < len(bb3d_sizes):\n",
    "                            start_idx = batch_id * MAX_BOXES_PER_BATCH\n",
    "                            end_idx = min(len(bb3d_sizes), start_idx + MAX_BOXES_PER_BATCH)\n",
    "                            rr.log(\n",
    "                                f\"world/evl_3d_bboxes/batch_{batch_id}\",\n",
    "                                rr.Boxes3D(\n",
    "                                    sizes=bb3d_sizes[start_idx:end_idx],\n",
    "                                    centers=bb3d_centers[start_idx:end_idx],\n",
    "                                    rotations=bb3d_quats_xyzw[start_idx:end_idx],\n",
    "                                    labels=bb3d_labels[start_idx:end_idx],\n",
    "                                    colors=[0, 255, 0, 70],  # Green with alpha from plot_style.py EVL_BBOX_3D\n",
    "                                    radii=0.005,  # From plot_style.py EVL_BBOX_3D plot_3d_size\n",
    "                                    show_labels=False,\n",
    "                                )\n",
    "                            )\n",
    "                            batch_id += 1\n",
    "    rr.notebook_show()\n",
    "else:\n",
    "    print(\"Skipping EVL visualization - no EVL data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation Stereo Depth\n",
    "\n",
    "Foundation Stereo provides depth estimation from stereo camera pairs, including depth maps and rectified images.\n",
    "\n",
    "### Foundation Stereo Data Structure\n",
    "\n",
    "The `CameraIntrinsicsAndPose` class contains:\n",
    "\n",
    "| Field Name | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `timestamp_ns` | int | Timestamp in device time domain (nanoseconds) |\n",
    "| `camera_projection` | CameraProjection | Camera intrinsics and model information |\n",
    "| `transform_world_camera` | SE3 | Camera pose in world coordinates |\n",
    "\n",
    "**Depth Map Format:**\n",
    "- Rectified depth maps of slam-front-left camera, 512 x 512, 16-bit grayscale PNG(1 unit = 1mm). \n",
    "\n",
    "**Rectified SLAM Image:**\n",
    "- Matching rectified slam-front-left camera images, 8-bit grayscale PNG.\n",
    "\n",
    "### Foundation Stereo API Reference\n",
    "\n",
    "- `has_stereo_depth_data()`: Check if stereo depth data is available\n",
    "- `get_stereo_depth_depth_map_by_index(index)`: Get depth map by index\n",
    "- `get_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)`: Get depth map by timestamp\n",
    "- `get_stereo_depth_rectified_slam_front_left_by_index(index)`: Get rectified image by index\n",
    "- `get_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)`: Get rectified image by timestamp\n",
    "- `get_stereo_depth_camera_intrinsics_and_pose_by_index(index)`: Get camera info by index\n",
    "- `get_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)`: Get camera info by timestamp\n",
    "- `get_stereo_depth_data_total_number()`: Get total number of depth entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation Stereo Depth Data Loading and Analysis\n",
    "if pilot_data_provider.has_stereo_depth_data():\n",
    "    print(\"✅ Foundation Stereo data is available\")\n",
    "    \n",
    "    # Get total number of stereo depth entries\n",
    "    total_stereo = pilot_data_provider.get_stereo_depth_data_total_number()\n",
    "    print(f\"Total stereo depth entries: {total_stereo}\")\n",
    "    \n",
    "    # Sample first few stereo depth entries\n",
    "    print(\"\\n=== Foundation Stereo Data Sample ===\")\n",
    "    sample_count = min(3, total_stereo)\n",
    "    for i in range(sample_count):\n",
    "        # Get depth map\n",
    "        depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_index(i)\n",
    "        \n",
    "        # Get rectified image\n",
    "        rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_index(i)\n",
    "        \n",
    "        # Get camera info\n",
    "        camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_index(i)\n",
    "        \n",
    "        if depth_map is not None:\n",
    "            print(f\"Entry {i}:\")\n",
    "            print(f\"  Depth map shape: {depth_map.shape}, dtype: {depth_map.dtype}\")\n",
    "            print(f\"  Depth range: {depth_map[depth_map > 0].min()}-{depth_map[depth_map > 0].max()} mm\")\n",
    "            print(f\"  Valid pixels: {np.sum(depth_map > 0)}/{depth_map.size} ({100*np.sum(depth_map > 0)/depth_map.size:.1f}%)\")\n",
    "            \n",
    "            if rectified_image is not None:\n",
    "                print(f\"  Rectified image shape: {rectified_image.shape}\")\n",
    "            \n",
    "            if camera_info is not None:\n",
    "                print(f\"  Camera model: {camera_info.camera_projection.model_name()}\")\n",
    "                print(f\"  Focal lengths: {camera_info.camera_projection.get_focal_lengths()}\")\n",
    "                print(f\"  Principal point: {camera_info.camera_projection.get_principal_point()}\")\n",
    "                print(f\"  Projection params: {camera_info.camera_projection.projection_params()}\")\n",
    "    \n",
    "    # Query stereo depth data by timestamp\n",
    "    if total_stereo > 0:\n",
    "        slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"slam-front-left\")\n",
    "        sample_timestamps = []\n",
    "        for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2):\n",
    "            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n",
    "            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n",
    "        \n",
    "        if sample_timestamp is not None:\n",
    "            # Query depth map at this timestamp\n",
    "            depth_at_time = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns(\n",
    "                sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    "            )\n",
    "            \n",
    "            if depth_at_time is not None:\n",
    "                print(f\"\\nDepth map at timestamp {sample_timestamp}:\")\n",
    "                print(f\"  Shape: {depth_at_time.shape}\")\n",
    "                print(f\"  Valid depth range: {depth_at_time[depth_at_time > 0].min()}-{depth_at_time[depth_at_time > 0].max()} mm\")\n",
    "else:\n",
    "    print(\"❌ Foundation Stereo data is not available in this sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation Stereo Depth Visualization\n",
    "if pilot_data_provider.has_stereo_depth_data():\n",
    "    print(\"\\n=== Visualizing Foundation Stereo Depth Data ===\")\n",
    "    \n",
    "    # Initialize Rerun for visualization\n",
    "    rr.init(\"rerun_viz_stereo_depth\")\n",
    "    \n",
    "    # Get total number of stereo depth entries\n",
    "    total_stereo = pilot_data_provider.get_stereo_depth_data_total_number()\n",
    "    \n",
    "    slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"slam-front-left\")\n",
    "    sample_timestamps = []\n",
    "    for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2):\n",
    "        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n",
    "        sample_timestamps.append(rgb_record.capture_timestamp_ns)\n",
    "    \n",
    "    for query_timestamp_ns in sample_timestamps:\n",
    "        # Get depth map\n",
    "        depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n",
    "        \n",
    "        # Get rectified image\n",
    "        rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n",
    "        \n",
    "        # Get camera info\n",
    "        camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n",
    "        \n",
    "        if depth_map is not None and rectified_image is not None and camera_info is not None:\n",
    "            # Set timestamp\n",
    "            rr.set_time_nanos(\"device_time\", camera_info.timestamp_ns)\n",
    "            \n",
    "            # Clear previous depth visualizations \n",
    "            rr.log(\"depth_image\", rr.Clear.recursive())\n",
    "            rr.log(\"rectified_slam_front_left\", rr.Clear.recursive())\n",
    "            rr.log(\"world/stereo_depth_depth_camera\", rr.Clear.recursive())\n",
    "            \n",
    "            # Visualize rectified SLAM image (following visualizer pattern)\n",
    "            if rectified_image is not None:\n",
    "                rr.log(\"rectified_slam_front_left\", rr.Image(rectified_image))\n",
    "            \n",
    "            # Visualize depth as 3D point cloud\n",
    "            # Get original camera intrinsics\n",
    "            original_fx, original_fy = camera_info.camera_projection.get_focal_lengths()\n",
    "            original_ux, original_uy = camera_info.camera_projection.get_principal_point()\n",
    "            \n",
    "            # Apply downsampling factor (following visualizer logic)\n",
    "            factor = 4  # depth_image_downsample_factor\n",
    "            scaled_fx = original_fx / factor\n",
    "            scaled_fy = original_fy / factor\n",
    "            scaled_ux = original_ux / factor\n",
    "            scaled_uy = original_uy / factor\n",
    "            \n",
    "            # Resize depth map (following visualizer pattern)\n",
    "            subsampled_depth_map = depth_map[::factor, ::factor] if factor > 1 else depth_map\n",
    "            \n",
    "            # Set up depth camera in world coordinate system (following visualizer pattern)\n",
    "            rr.log(\n",
    "                \"world/stereo_depth\",\n",
    "                rr.Pinhole(\n",
    "                    resolution=[subsampled_depth_map.shape[1], subsampled_depth_map.shape[0]],\n",
    "                    focal_length=[scaled_fx, scaled_fy],\n",
    "                    principal_point=[scaled_ux, scaled_uy],\n",
    "                ),\n",
    "                static=True,\n",
    "            )\n",
    "            \n",
    "            # Log camera transform (following visualizer pattern)\n",
    "            rr.log(\n",
    "                \"world/stereo_depth\",\n",
    "                ToTransform3D(camera_info.transform_world_camera, axis_length=0.02)\n",
    "            )\n",
    "            \n",
    "            # Log depth image with proper scaling (following visualizer pattern exactly)\n",
    "            DEPTH_IMAGE_SCALING = 1000  # mm to meters\n",
    "            rr.log(\n",
    "                \"world/stereo_depth\",\n",
    "                rr.DepthImage(\n",
    "                    subsampled_depth_map,\n",
    "                    meter=DEPTH_IMAGE_SCALING,\n",
    "                    colormap=\"Magma\",\n",
    "                    point_fill_ratio=0.3 \n",
    "                )\n",
    "            )\n",
    "    rr.notebook_show()\n",
    "else:\n",
    "    print(\"Skipping stereo depth visualization - no stereo depth data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial has demonstrated how to use the `AriaGen2PilotDataProvider` to access and visualize algorithm output data from the Aria Gen2 Pilot Dataset:\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Heart Rate Monitoring** - Physiological data from PPG sensors with time series visualization\n",
    "2. **Diarization** - Speaker identification and voice activity detection with text overlay\n",
    "3. **Hand-Object Interaction** - Segmentation masks for hands and objects with colored overlays\n",
    "4. **Egocentric Voxel Lifting** - 3D scene reconstruction with 2D/3D bounding box visualization\n",
    "5. **Foundation Stereo** - Depth estimation with 2D depth maps and 3D point clouds\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- **Data Availability**: Algorithm data availability varies by sequence - always check availability before processing\n",
    "- **Data Structures**: Each algorithm has its own data structure with specific fields and formats\n",
    "- **Query Patterns**: Use index-based queries for sequential processing, timestamp-based queries for synchronization\n",
    "- **Visualization**: Use appropriate visualization methods for each data type (scalars, images, bounding box, etc.)\n",
    "- **Performance**: Consider subsampling for large datasets and high-frequency data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npd",
   "language": "python",
   "name": "npd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
