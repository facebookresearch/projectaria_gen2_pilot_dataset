{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading\n",
    "\n",
    "This tutorial demonstrates how to load and visualize MPS data from the Aria Gen2 Pilot Dataset using the `AriaGen2PilotDataProvider`.\n",
    "    \n",
    "## Introduction\n",
    "\n",
    "Machine Perception Services, or MPS, is a post-processing cloud service that we provide to Aria users. \n",
    "It runs a set of proprietary Spatial AI machine perception algorithms that are designed for Project Aria glasses. \n",
    "MPS is designed to provide superior accuracy and robustness compared to off-the-shelf open algorithms. \n",
    "\n",
    "Currently, the supported MPS algorithms for Aria Gen2 include: \n",
    "1. SLAM: Single Sequence Trajectory and Semi-dense point cloud generation.\n",
    "2. Hand Tracking: 21 landmarks, wrist to device transformation, wrist and palm positions and normals.\n",
    "\n",
    "This tutorial focuses on demonstrating how to load and visualize the MPS results. \n",
    "\n",
    "\n",
    "**What You'll Learn**\n",
    "\n",
    "1. How to load MPS output data, and definitions of the data types. \n",
    "2. How to visualize the MPS data\n",
    "3. Understanding the difference between MPS and on-device perception outputs\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "The Aria Gen2 Pilot Dataset comprises four primary data content types:\n",
    "1. Raw sensor streams acquired directly from Aria Gen2 devices\n",
    "2. Real-time machine perception outputs generated on-device via embedded algorithms during data collection\n",
    "3. **Offline machine perception results produced by Machine Perception Services (MPS) during post-processing** (focus of this tutorial)\n",
    "4. Outputs from additional offline perception algorithms\n",
    "\n",
    "Each sequence folder contains an `mps/` directory with the following structure:\n",
    "```\n",
    "mps/\n",
    "├── slam/\n",
    "│   ├── closed_loop_trajectory.csv\n",
    "│   ├── open_loop_trajectory.csv\n",
    "│   ├── semidense_observations.csv.gz\n",
    "│   ├── semidense_points.csv.gz\n",
    "│   ├── online_calibration.jsonl\n",
    "│   └── summary.json\n",
    "└── hand_tracking/\n",
    "    ├── hand_tracking_results.csv\n",
    "    └── summary.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Provider\n",
    "\n",
    "**⚠️ Important:** Update the `sequence_path` below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\n",
    "from projectaria_tools.core.sensor_data import TimeDomain\n",
    "from projectaria_tools.core import mps\n",
    "from aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity, PlotStyle\n",
    "from projectaria_tools.utils.rerun_helpers import (\n",
    "    create_hand_skeleton_from_landmarks,\n",
    "    ToTransform3D,\n",
    ")\n",
    "import rerun as rr\n",
    "import rerun.blueprint as rrb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your dataset location\n",
    "sequence_path = \"path/to/your/sequence_folder\"\n",
    "pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS - SLAM\n",
    "### [MPS - SLAM] Output Files\n",
    "MPS output result files are categorized into sub-folders by algorithm. \n",
    "For SLAM algorithm output, it generates the following files: \n",
    "- `closed_loop_trajectory.csv`\n",
    "- `open_loop_trajectory.csv`\n",
    "- `semidense_observations.csv.gz`\n",
    "- `semidense_points.csv.gz`\n",
    "- `online_calibration.jsonl`\n",
    "- `summary.json`\n",
    "\n",
    "Please refer to the [MPS Wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam) for details of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPS - SLAM] Semi-dense Point Cloud and Observations\n",
    "\n",
    "The MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud) for data type definitions): \n",
    "- `semidense_points.csv.gz`: Global points in the world coordinate frame. \n",
    "- `semidense_observations.csv.gz`: Point observations for each camera, at each timestamp.\n",
    "\n",
    "Note that semidense point files are normally large, therefore loading them may take some time. \n",
    "\n",
    "#### Point Cloud Data Types\n",
    "\n",
    "**GlobalPointPosition** contains:\n",
    "- `uid`: Unique identifier for the 3D point\n",
    "- `graph_uid`: Identifier linking point to pose graph\n",
    "- `position_world`: 3D position in world coordinate frame\n",
    "- `inverse_distance_std`: Inverse distance standard deviation (quality metric)\n",
    "- `distance_std`: Distance standard deviation (quality metric)\n",
    "\n",
    "**PointObservation** contains:\n",
    "- `point_uid`: Links observation to 3D point\n",
    "- `frame_capture_timestamp`: When the observation was captured\n",
    "- `camera_serial`: Serial number of the observing camera\n",
    "- `uv`: 2D pixel coordinates of the observation\n",
    "\n",
    "#### Filtering Point Cloud by Confidence\n",
    "\n",
    "You can filter the semi-dense point cloud using confidence thresholds based on `inverse_distance_std` or `distance_std` to improve quality. The `AriaGen2PilotDataProvider` provides a convenient method `get_mps_semidense_point_cloud_filtered()` for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Semi-dense Point Cloud ===\")\n",
    "\n",
    "semi_dense_point_cloud = pilot_data_provider.get_mps_semidense_point_cloud()\n",
    "semi_dense_point_cloud_filtered = pilot_data_provider.get_mps_semidense_point_cloud_filtered(filter_confidence=True, max_point_count=50000)\n",
    "\n",
    "# Print out the content of the first sample in semidense_points\n",
    "if semi_dense_point_cloud:\n",
    "    sample = semi_dense_point_cloud[0]\n",
    "    print(\"GlobalPointPosition sample:\")\n",
    "    print(f\"  uid: {sample.uid}\")\n",
    "    print(f\"  graph_uid: {sample.graph_uid}\")\n",
    "    print(f\"  position_world: {sample.position_world}\")\n",
    "    print(f\"  inverse_distance_std: {sample.inverse_distance_std}\")\n",
    "    print(f\"  distance_std: {sample.distance_std}\")\n",
    "    print(f\"Total number of semi-dense points: {len(semi_dense_point_cloud)}\")\n",
    "    print(f\"Total number of filtered semi-dense points: {len(semi_dense_point_cloud_filtered)}\")\n",
    "else:\n",
    "    print(\"semidense_points is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPS - SLAM] Closed vs Open Loop Trajectory\n",
    "\n",
    "The MPS SLAM algorithm outputs 2 trajectory files `open_loop_trajectory.csv` and `closed_loop_trajectory.csv` (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory) for data type definitions): \n",
    "\n",
    "- **Open loop trajectory**: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance.\n",
    "\n",
    "- **Closed loop trajectory**: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans. For closed loop, an additional interpolation API is provided. \n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "| Aspect | Open Loop (VIO) | Closed Loop (SLAM) |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Coordinate Frame** | Odometry frame | Global world frame |\n",
    "| **Drift** | Accumulates over time | Minimized with loop closure |\n",
    "| **Accuracy** | Good for short periods | Higher global accuracy |\n",
    "| **Use Case** | Real-time odometry | High-quality reconstruction |\n",
    "\n",
    "#### Data Types\n",
    "\n",
    "**ClosedLoopTrajectoryPose** contains:\n",
    "- `tracking_timestamp`: Device timestamp when pose was computed\n",
    "- `transform_world_device`: 6DOF pose in world coordinate frame\n",
    "- `device_linear_velocity_device`: Linear velocity in device frame\n",
    "- `angular_velocity_device`: Angular velocity in device frame\n",
    "- `quality_score`: Pose estimation quality (higher = better)\n",
    "- `gravity_world`: Gravity vector in world frame\n",
    "\n",
    "**OpenLoopTrajectoryPose** contains:\n",
    "- `tracking_timestamp`: Device timestamp when pose was computed\n",
    "- `transform_odometry_device`: 6DOF pose in odometry coordinate frame\n",
    "- `device_linear_velocity_odometry`: Linear velocity in odometry frame\n",
    "- `angular_velocity_device`: Angular velocity in device frame\n",
    "- `quality_score`: Pose estimation quality (higher = better)\n",
    "- `gravity_odometry`: Gravity vector in odometry frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - query whole trajectory ===\")\n",
    "mps_closed_loop_trajectory = pilot_data_provider.get_mps_closed_loop_trajectory()\n",
    "mps_closed_loop_trajectory_duration = mps_closed_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds()\n",
    "print(\"MPS Closed Loop Trajectory duration: \" + f\"{mps_closed_loop_trajectory_duration:.2f}\")\n",
    "\n",
    "mps_open_loop_trajectory = pilot_data_provider.get_mps_open_loop_trajectory()\n",
    "mps_open_loop_trajectory_duration = mps_open_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_open_loop_trajectory[0].tracking_timestamp.total_seconds()\n",
    "print(\"MPS Open Loop Trajectory duration: \" + f\"{mps_open_loop_trajectory_duration:.2f}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=== MPS - query pose by timestamp ===\")\n",
    "\n",
    "query_timestamp_ns = int((mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9)\n",
    "print(\"query timestamp ns:\", query_timestamp_ns, '\\n')\n",
    "if mps_closed_loop_trajectory_duration > 1:  # If duration > 1s\n",
    "\n",
    "    nearest_mps_closed_loop_pose = pilot_data_provider.get_mps_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n",
    "    print(\"Nearest mps closed loop pose: \", nearest_mps_closed_loop_pose, \"\\n\")\n",
    "\n",
    "    interpolated_mps_closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n",
    "    print(\"Interpolated mps closed loop pose: \", interpolated_mps_closed_loop_pose,  \"\\n\")\n",
    "\n",
    "if mps_open_loop_trajectory_duration > 1:  # If duration > 1s\n",
    "    query_timestamp_ns = int((mps_open_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9)\n",
    "    nearest_mps_open_loop_pose = pilot_data_provider.get_mps_open_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n",
    "    print(\"Nearest mps closed loop pose: \", nearest_mps_open_loop_pose,  \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MPS - Hand Tracking]\n",
    "\n",
    "The MPS Hand Tracking algorithm outputs 2 files related to hand-tracking: `hand_tracking_results.csv` and `summary.json`. See [wiki page](TODO: update link when available) for data type definitions.\n",
    "\n",
    "### Hand Tracking Features\n",
    "\n",
    "MPS Hand Tracking provides the following outputs for each detected hand:\n",
    "- **21 landmarks**: Detailed hand joint positions\n",
    "- **Wrist to device transformation**: Spatial relationship between wrist and device\n",
    "- **Wrist and palm positions**: Key reference points for hand pose\n",
    "- **Palm normals**: Surface orientation for interaction analysis\n",
    "\n",
    "### Query Methods\n",
    "\n",
    "The `AriaGen2PilotDataProvider` offers two query methods for hand tracking data:\n",
    "- `get_mps_hand_tracking_result()`: Returns the nearest hand tracking result for a given timestamp\n",
    "- `get_mps_interpolated_hand_tracking_result()`: Returns interpolated hand tracking result for smoother motion analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - query whole hand tracking results ===\")\n",
    "mps_hand_tracking_results = pilot_data_provider.get_mps_hand_tracking_result_list()\n",
    "mps_hand_tracking_results_duration = mps_hand_tracking_results[-1].tracking_timestamp.total_seconds() - mps_hand_tracking_results[0].tracking_timestamp.total_seconds()\n",
    "print(\"MPS hand tracking results duration: \" + f\"{mps_hand_tracking_results_duration:.2f}\", \"\\n\")\n",
    "\n",
    "print(\"=== MPS - query hand tracking result by timestamp ===\")\n",
    "if mps_hand_tracking_results_duration > 1:  # If duration > 1s\n",
    "    query_timestamp_ns = int((mps_hand_tracking_results[0].tracking_timestamp.total_seconds()+1)*1e9)\n",
    "    print(\"Query timestamp ns:\", query_timestamp_ns, '\\n')\n",
    "    nearest_mps_hand_tracking_result = pilot_data_provider.get_mps_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n",
    "    print(\"Nearest tracking result: \", nearest_mps_hand_tracking_result, \"\\n\")\n",
    "    interpolated_mps_hand_tracking_result = pilot_data_provider.get_mps_interpolated_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n",
    "    print(\"Interpolated tracking result: \", interpolated_mps_hand_tracking_result, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The following section shows how to plot: \n",
    "- Semi-dense point cloud\n",
    "- Closed loop trajectory\n",
    "- Hand pose results\n",
    "\n",
    "### Visualization Notes\n",
    "\n",
    "- **Point Cloud Filtering**: For better visualization performance, we filter the point cloud by confidence and limit the maximum point count\n",
    "- **Trajectory Caching**: We accumulate trajectory points over time to visualize the full path\n",
    "- **Rerun Integration**: We use Rerun for interactive 3D visualization with proper coordinate frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "def plot_mps_semidense_point_cloud(\n",
    "        point_cloud_data: list[mps.GlobalPointPosition]\n",
    "    ) -> None:\n",
    "        if point_cloud_data == []:\n",
    "            return\n",
    "        points_array = np.array(\n",
    "            [\n",
    "                point.position_world\n",
    "                for point in point_cloud_data\n",
    "                if hasattr(point, \"position_world\")\n",
    "            ]\n",
    "        )\n",
    "        plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD)\n",
    "        rr.log(\n",
    "            f\"world/{plot_style.label}\",\n",
    "            rr.Points3D(\n",
    "                positions=points_array,\n",
    "                colors=[] * len(points_array),\n",
    "                radii=plot_style.plot_3d_size,\n",
    "            ),\n",
    "            static=True,\n",
    "        )\n",
    "\n",
    "def plot_closed_loop_pose(\n",
    "        closed_loop_trajectory_pose: mps.ClosedLoopTrajectoryPose,\n",
    "    ) -> None:\n",
    "        \"\"\"Plot MPS closed loop trajectory\"\"\"\n",
    "        if not closed_loop_trajectory_pose:\n",
    "            return\n",
    "        # Get transform and add to trajectory cache\n",
    "        T_world_device = closed_loop_trajectory_pose.transform_world_device\n",
    "        closed_loop_trajectory_pose_cache.append(T_world_device.translation()[0])\n",
    "\n",
    "        # Plot device pose\n",
    "        rr.log(\n",
    "            \"world/device\",\n",
    "            ToTransform3D(T_world_device, axis_length=0.05),\n",
    "        )\n",
    "\n",
    "        # Plot accumulated trajectory\n",
    "        if len(closed_loop_trajectory_pose_cache) > 1:\n",
    "            plot_style = get_plot_style(PlotEntity.TRAJECTORY)\n",
    "            rr.log(\n",
    "                f\"world/{plot_style.label}\",\n",
    "                rr.LineStrips3D(\n",
    "                    [closed_loop_trajectory_pose_cache],\n",
    "                    colors=[plot_style.color],\n",
    "                    radii=plot_style.plot_3d_size,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "def _get_hand_plot_style(hand_label: str) -> PlotStyle:\n",
    "    if hand_label == \"left\":\n",
    "        landmarks_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS\n",
    "        skeleton_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON\n",
    "    else:\n",
    "        landmarks_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS\n",
    "        skeleton_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON\n",
    "\n",
    "    return get_plot_style(landmarks_plot_entity), get_plot_style(\n",
    "        skeleton_plot_entity\n",
    "    )\n",
    "\n",
    "def _plot_single_hand_3d(\n",
    "    hand_joints_in_device: list[np.array],\n",
    "    hand_label: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot single hand data in 3D and 2D camera views\n",
    "    \"\"\"\n",
    "    landmarks_style, skeleton_style = _get_hand_plot_style(hand_label=hand_label)\n",
    "    if hand_joints_in_device is None:\n",
    "        return\n",
    "    # Plot 3D hand markers and skeleton\n",
    "    hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device)\n",
    "    rr.log(\n",
    "        f\"world/device/hand-tracking/{hand_label}/{landmarks_style.label}\",\n",
    "        rr.Points3D(\n",
    "            positions=hand_joints_in_device,\n",
    "            colors=[landmarks_style.color],\n",
    "            radii=landmarks_style.plot_3d_size,\n",
    "        ),\n",
    "    )\n",
    "    rr.log(\n",
    "        f\"world/device/hand-tracking/{hand_label}/{skeleton_style.label}\",\n",
    "        rr.LineStrips3D(\n",
    "            hand_skeleton_3d,\n",
    "            colors=[skeleton_style.color],\n",
    "            radii=skeleton_style.plot_3d_size,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def plot_mps_hand_tracking_result_3d(\n",
    "        hand_pose_data: mps.hand_tracking.HandTrackingResult,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot hand pose data within 3D world view\n",
    "        \"\"\"\n",
    "        rr.log(\n",
    "            \"world/device/hand-tracking\",\n",
    "            rr.Clear.recursive(),\n",
    "        )\n",
    "        if hand_pose_data is None:\n",
    "            return\n",
    "\n",
    "        if hand_pose_data.left_hand is not None:\n",
    "            _plot_single_hand_3d(\n",
    "                hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device,\n",
    "                hand_label=\"left\",\n",
    "            )\n",
    "        if hand_pose_data.right_hand is not None:\n",
    "            _plot_single_hand_3d(\n",
    "                hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device,\n",
    "                hand_label=\"right\",\n",
    "            )\n",
    "\n",
    "closed_loop_trajectory_pose_cache=[]\n",
    "open_loop_trajectory_pose_cache=[]\n",
    "\n",
    "def plot_mps():\n",
    "    rr.init(\"rerun_viz_mps\")\n",
    "    # Create a Spatial3D view to display the points.\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Spatial3DView(\n",
    "            origin=\"/\",\n",
    "            name=\"3D Scene\",\n",
    "            # Set the background color to light blue.\n",
    "            background=[0, 0, 0],\n",
    "        ),\n",
    "        collapse_panels=True,\n",
    "    )\n",
    "\n",
    "    # plot semi-dense point cloud\n",
    "    plot_mps_semidense_point_cloud(semi_dense_point_cloud_filtered)\n",
    "    for hand_tracking_result in mps_hand_tracking_results:\n",
    "        closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(int(hand_tracking_result.tracking_timestamp.total_seconds()*1e9), TimeDomain.DEVICE_TIME)\n",
    "\n",
    "        # plot hand tracking result\n",
    "        plot_mps_hand_tracking_result_3d(hand_tracking_result)\n",
    "\n",
    "        # plot closed loop pose\n",
    "        plot_closed_loop_pose(closed_loop_pose)\n",
    "    rr.notebook_show(blueprint=blueprint)\n",
    "\n",
    "plot_mps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered the essential aspects of working with MPS data in the Aria Gen2 Pilot Dataset:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MPS SLAM Trajectories**: Understanding the difference between open loop (VIO-based, with drift) and closed loop (globally optimized, minimal drift) trajectories\n",
    "2. **Semi-dense Point Cloud**: Accessing high-quality 3D reconstructions with confidence-based filtering\n",
    "3. **Hand Tracking**: Loading and visualizing detailed hand pose data with 21 landmarks per hand\n",
    "4. **Data Querying**: Using both nearest-neighbor and interpolated queries for smooth temporal access\n",
    "5. **3D Visualization**: Creating interactive visualizations with Rerun\n",
    "\n",
    "### MPS vs On-Device Data\n",
    "\n",
    "| Aspect | On-Device (VIO/Tracking) | MPS (Post-processing) |\n",
    "|--------|--------------------------|----------------------|\n",
    "| **Processing** | Real-time during recording | Cloud-based offline processing |\n",
    "| **Accuracy** | Good for real-time use | Higher accuracy with global optimization |\n",
    "| **Latency** | Immediate availability | Requires post-processing time |\n",
    "| **Drift** | Accumulates over time | Minimized with loop closure (SLAM) |\n",
    "| **Point Cloud** | Not available | Dense semi-dense reconstructions |\n",
    "| **Coordinate Frame** | Odometry/device frame | Global world frame |\n",
    "| **Use Cases** | Live feedback, real-time apps | Research, high-quality reconstruction, benchmarking |"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "e16e7590-512b-4ca5-b6b7-b01788d4ba4a",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
