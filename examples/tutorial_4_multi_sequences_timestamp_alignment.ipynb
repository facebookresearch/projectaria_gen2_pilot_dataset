{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to work with timestamp-aligned multi-device Aria Gen2 recordings. When multiple Aria Gen2 glasses record simultaneously with SubGHz timestamp alignment enabled, their timestamps can be mapped across devices, enabling multi-person activity analysis, multi-view reconstruction, and collaborative tasks.\n",
    "\n",
    "**What You'll Learn:**\n",
    "\n",
    "- Understanding SubGHz time timestamp alignment between multiple Aria Gen2 devices\n",
    "- Converting timestamps between host and client devices\n",
    "- Querying timestamp-aligned sensor data across multiple recordings\n",
    "- Visualizing timestamp-aligned RGB frames from multiple devices\n",
    "- Plotting trajectories and hand tracking from multiple devices in a shared world coordinate frame\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete Tutorial 1 (VRS Data Loading) to understand basic data provider concepts\n",
    "- Complete Tutorial 2 (MPS Data Loading) to understand MPS trajectories and hand tracking\n",
    "- Download a multi-device sequence from the Aria Gen2 Pilot Dataset\n",
    "\n",
    "## SubGHz Timestamp Alignment Overview\n",
    "\n",
    "During multi-device recording, Aria Gen2 glasses use SubGHz radio signals for timestamp alignment:\n",
    "\n",
    "- **Host Device**: One device acts as the host, actively broadcasting SubGHz signals to a specified channel\n",
    "- **Client Device(s)**: Other devices act as clients, receiving SubGHz signals and recording a `Time Domain Mapping` data stream in their VRS file\n",
    "\n",
    "**Important Notes:**\n",
    "- The time domain mapping stream **only exists in client VRS files**, not in the host VRS\n",
    "- This mapping enables converting timestamps: host `DEVICE_TIME` ↔ client `DEVICE_TIME`\n",
    "- MPS trajectories from timestamp-aligned recordings share the same world coordinate frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Project Aria Tools imports\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions, TimeSyncMode\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.utils.rerun_helpers import (\n",
    "    create_hand_skeleton_from_landmarks,\n",
    "    ToTransform3D\n",
    ")\n",
    "\n",
    "# Aria Gen2 Pilot Dataset imports\n",
    "from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\n",
    "from aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity\n",
    "\n",
    "# Visualization library\n",
    "import rerun as rr\n",
    "import rerun.blueprint as rrb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Single-Device Timestamp Alignment\n",
    "\n",
    "Before diving into multi-device synchronization, let's understand how timestamp alignment works within a single Aria Gen2 device.\n",
    "\n",
    "### Understanding Time Domains\n",
    "\n",
    "In `projectaria_tools`, every timestamp is linked to a specific `TimeDomain`, which represents the time reference or clock used to generate that timestamp. Timestamps from different `TimeDomain`s are not directly comparable—only timestamps within the same `TimeDomain` are consistent and can be accurately compared or aligned.\n",
    "\n",
    "#### Supported Time Domains for Aria Gen2\n",
    "\n",
    "> **Important: Use `DEVICE_TIME` for single-device Aria data analysis**\n",
    "\n",
    "| Time Domain | Description | Usage |\n",
    "|-------------|-------------|-------|\n",
    "| **DEVICE_TIME (Recommended)**| Capture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain. | **Use this for single-device Aria data analysis** |\n",
    "| **RECORD_TIME** | Timestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point. | Fast access, but use DEVICE_TIME for accuracy |\n",
    "| **HOST_TIME** | Timestamps when sensor data is saved to the device (not when captured). | Should not be needed for any purpose |\n",
    "\n",
    "**For multi-device time alignment** (covered in Part 2), we use:\n",
    "- **SUBGHZ**: Multi-device time alignment for Aria Gen2 using SubGHz signals\n",
    "\n",
    "### Load a Single Sequence\n",
    "\n",
    "Let's start by loading a single Aria Gen2 sequence to demonstrate timestamp-based queries:\n",
    "\n",
    "**⚠️ Important:** Update the path below to point to your downloaded sequence folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your dataset location\n",
    "sequence_path = \"path/to/your/sequence_folder\"\n",
    "\n",
    "# Initialize data provider\n",
    "print(\"Loading sequence data...\")\n",
    "pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data Loaded Successfully!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data API to Query by Timestamp\n",
    "\n",
    "The data provider offers powerful timestamp-based data access through the `get_vrs_$SENSOR_data_by_time_ns()` API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval.\n",
    "\n",
    "For any sensor type, you can query data by timestamp using these functions:\n",
    "- `get_vrs_image_data_by_time_ns()` - Query image data (RGB, SLAM cameras)\n",
    "- `get_vrs_imu_data_by_time_ns()` - Query IMU data\n",
    "- `get_vrs_audio_data_by_time_ns()` - Query audio data\n",
    "- And more...\n",
    "\n",
    "#### TimeQueryOptions\n",
    "\n",
    "The `TimeQueryOptions` parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:\n",
    "\n",
    "| Option | Behavior | Use Case |\n",
    "|--------|----------|----------|\n",
    "| **BEFORE** | Returns the last valid data with `timestamp ≤ query_time` | **Default and most common** - Get the most recent data before or at the query time |\n",
    "| **AFTER** | Returns the first valid data with `timestamp ≥ query_time` | Get the next available data after or at the query time |\n",
    "| **CLOSEST** | Returns data with smallest `|timestamp - query_time|` | Get the temporally closest data regardless of direction |\n",
    "\n",
    "#### Boundary Behavior\n",
    "\n",
    "The API handles edge cases automatically:\n",
    "\n",
    "| Query Condition | BEFORE | AFTER | CLOSEST |\n",
    "|-----------------|--------|-------|---------| \n",
    "| `query_time < first_timestamp` | Returns invalid data | Returns first data | Returns first data |\n",
    "| `first_timestamp ≤ query_time ≤ last_timestamp` | Returns data with `timestamp ≤ query_time` | Returns data with `timestamp ≥ query_time` | Returns temporally closest data |\n",
    "| `query_time > last_timestamp` | Returns last data | Returns invalid data | Returns last data |\n",
    "\n",
    "Let's demonstrate timestamp-based queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Single Device Timestamp-Based Query Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select RGB stream ID\n",
    "rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "\n",
    "# Get a timestamp within the recording (3 seconds after start)\n",
    "start_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\n",
    "selected_timestamp_ns = start_timestamp_ns + int(3e9)\n",
    "\n",
    "print(f\"\\nQuery timestamp: {selected_timestamp_ns} ns (3 seconds after start)\")\n",
    "\n",
    "# Fetch the RGB frame that is CLOSEST to this selected timestamp_ns\n",
    "closest_rgb_data, closest_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "    stream_id=rgb_stream_id,\n",
    "    time_ns=selected_timestamp_ns,\n",
    "    time_domain=TimeDomain.DEVICE_TIME,\n",
    "    time_query_options=TimeQueryOptions.CLOSEST\n",
    ")\n",
    "closest_timestamp_ns = closest_rgb_record.capture_timestamp_ns\n",
    "closest_frame_number = closest_rgb_record.frame_number\n",
    "print(f\"\\n✅ CLOSEST frame to query timestamp:\")\n",
    "print(f\"   Frame #{closest_frame_number}\")\n",
    "print(f\"   Capture timestamp: {closest_timestamp_ns} ns\")\n",
    "print(f\"   Time difference: {abs(closest_timestamp_ns - selected_timestamp_ns) / 1e6:.2f} ms\")\n",
    "\n",
    "# Fetch the frame BEFORE this frame\n",
    "prev_rgb_data, prev_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "    stream_id=rgb_stream_id,\n",
    "    time_ns=closest_timestamp_ns - 1,\n",
    "    time_domain=TimeDomain.DEVICE_TIME,\n",
    "    time_query_options=TimeQueryOptions.BEFORE\n",
    ")\n",
    "prev_timestamp_ns = prev_rgb_record.capture_timestamp_ns\n",
    "prev_frame_number = prev_rgb_record.frame_number\n",
    "print(f\"\\n⬅️ BEFORE frame:\")\n",
    "print(f\"   Frame #{prev_frame_number}\")\n",
    "print(f\"   Capture timestamp: {prev_timestamp_ns} ns\")\n",
    "\n",
    "# Fetch the frame AFTER this frame\n",
    "next_rgb_data, next_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "    stream_id=rgb_stream_id,\n",
    "    time_ns=closest_timestamp_ns + 1,\n",
    "    time_domain=TimeDomain.DEVICE_TIME,\n",
    "    time_query_options=TimeQueryOptions.AFTER\n",
    ")\n",
    "next_timestamp_ns = next_rgb_record.capture_timestamp_ns\n",
    "next_frame_number = next_rgb_record.frame_number\n",
    "print(f\"\\n➡️ AFTER frame:\")\n",
    "print(f\"   Frame #{next_frame_number}\")\n",
    "print(f\"   Capture timestamp: {next_timestamp_ns} ns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Timestamp-Aligned Multi-Sensor Data\n",
    "\n",
    "A common use case is to query and visualize data from multiple sensors at approximately the same timestamp. Let's demonstrate querying RGB and SLAM camera images at the same time:\n",
    "\n",
    "**Use Case:** Get RGB + SLAM images at 5Hz to see timestamp-aligned multi-camera views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Multi-Sensor Synchronized Query Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize a simple Rerun viewer for single-device visualization\n",
    "rr.init(\"single_device_sync_demo\", spawn=False)\n",
    "\n",
    "# Get stream IDs for RGB and SLAM cameras\n",
    "all_labels = pilot_data_provider.vrs_data_provider.get_device_calibration().get_camera_labels()\n",
    "slam_labels = [label for label in all_labels if \"slam\" in label]\n",
    "slam_stream_ids = [pilot_data_provider.get_vrs_stream_id_from_label(label) for label in slam_labels]\n",
    "rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "\n",
    "# Starting from +3 seconds into the recording, sample at 5Hz for 10 frames\n",
    "target_period_ns = int(2e8)  # 200ms = 5 Hz\n",
    "start_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9)\n",
    "\n",
    "print(f\"\\nQuerying RGB + SLAM images at 5Hz...\")\n",
    "print(f\"Starting from +3 seconds, sampling 10 frames\\n\")\n",
    "\n",
    "# Plot 10 samples\n",
    "current_timestamp_ns = start_timestamp_ns\n",
    "for frame_i in range(10):\n",
    "    # Set time for Rerun\n",
    "    rr.set_time_nanos(\"device_time\", current_timestamp_ns)\n",
    "\n",
    "    # Query and log RGB image\n",
    "    rgb_image_data, rgb_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "        stream_id=rgb_stream_id,\n",
    "        time_ns=current_timestamp_ns,\n",
    "        time_domain=TimeDomain.DEVICE_TIME,\n",
    "        time_query_options=TimeQueryOptions.CLOSEST\n",
    "    )\n",
    "    rr.log(\"single_device/rgb_image\", rr.Image(rgb_image_data.to_numpy_array()))\n",
    "\n",
    "    # Query and log SLAM images\n",
    "    for slam_i, (slam_label, slam_stream_id) in enumerate(zip(slam_labels, slam_stream_ids)):\n",
    "        slam_image_data, slam_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n",
    "            stream_id=slam_stream_id,\n",
    "            time_ns=current_timestamp_ns,\n",
    "            time_domain=TimeDomain.DEVICE_TIME,\n",
    "            time_query_options=TimeQueryOptions.CLOSEST\n",
    "        )\n",
    "        rr.log(f\"single_device/{slam_label}\", rr.Image(slam_image_data.to_numpy_array()))\n",
    "\n",
    "    if frame_i == 0:\n",
    "        print(f\"Frame {frame_i}: RGB timestamp {rgb_image_record.capture_timestamp_ns} ns\")\n",
    "\n",
    "    # Increment query timestamp\n",
    "    current_timestamp_ns += target_period_ns\n",
    "rr.notebook_show()\n",
    "print(f\"\\n✅ Successfully queried and logged 10 frames of synchronized multi-sensor data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Device Time Alignment\n",
    "\n",
    "Now that we understand single-device timestamp alignment, let's explore how to align timestamps across multiple Aria Gen2 devices using SubGHz signals.\n",
    "\n",
    "### Understanding Pilot Dataset Multi-Device Naming Convention\n",
    "\n",
    "In the Aria Gen2 Pilot Dataset, timestamp-aligned multi-device sequences share the same base name with different numeric suffixes:\n",
    "\n",
    "- **Host Device**: Sequences ending with `_0` (e.g., `eat_0`, `play_0`, `walk_0`)\n",
    "- **Client Devices**: Sequences ending with `_1`, `_2`, `_3`, etc. (e.g., `eat_1`, `eat_2`, `eat_3`)\n",
    "\n",
    "**Example:**\n",
    "- `eat_0` → Host device recording\n",
    "- `eat_1` → Client device 1 recording  \n",
    "- `eat_2` → Client device 2 recording\n",
    "- `eat_3` → Client device 3 recording\n",
    "\n",
    "All sequences with the same base name (e.g., all `eat_*`, `play_*`, `walk_*` sequences) share a SubGHz timestamp mapping and can be aligned using the time domain mapping.\n",
    "\n",
    "### Load Multiple Sequences\n",
    "\n",
    "We'll need data from both host and client devices. Make sure you have downloaded a multi-device sequence from the Aria Gen2 Pilot Dataset.\n",
    "\n",
    "**⚠️ Important:** Update the paths below to point to your downloaded host and client sequence folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these paths to your multi-device dataset location\n",
    "host_sequence_path = \"/path/to/host_sequence\"\n",
    "client_sequence_path = \"/path/to/client_sequence\"\n",
    "\n",
    "# Initialize data providers for both devices\n",
    "print(\"Loading host device data...\")\n",
    "host_data_provider = AriaGen2PilotDataProvider(host_sequence_path)\n",
    "\n",
    "print(\"Loading client device data...\")\n",
    "client_data_provider = AriaGen2PilotDataProvider(client_sequence_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Multi-Device Data Loaded Successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Host has MPS data: {'✅' if host_data_provider.has_mps_data() else '❌'}\")\n",
    "print(f\"Client has MPS data: {'✅' if client_data_provider.has_mps_data() else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SubGHz Timestamp Conversion\n",
    "\n",
    "The SubGHz synchronization creates a mapping between host and client device times. We can convert timestamps bidirectionally:\n",
    "\n",
    "- **Host → Client**: `client_vrs_provider.convert_from_synctime_to_device_time_ns(host_time, TimeSyncMode.SUBGHZ)`\n",
    "- **Client → Host**: `client_vrs_provider.convert_from_device_time_to_synctime_ns(client_time, TimeSyncMode.SUBGHZ)`\n",
    "\n",
    "**Important**: Both conversion functions are called on the **client's VRS data provider**, since only the client has the time domain mapping data.\n",
    "\n",
    "Let's demonstrate timestamp conversion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Multi-Device Timestamp Conversion Example\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get RGB stream ID from host\n",
    "rgb_stream_id = host_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "\n",
    "# Pick a timestamp in the middle of the host recording\n",
    "host_start_time = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "host_end_time = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "selected_host_timestamp_ns = (host_start_time + host_end_time) // 2\n",
    "\n",
    "print(f\"\\nSelected host timestamp: {selected_host_timestamp_ns} ns\")\n",
    "print(f\"  (Host recording spans {(host_end_time - host_start_time) / 1e9:.2f} seconds)\")\n",
    "\n",
    "# Convert from host time to client time\n",
    "converted_client_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_synctime_to_device_time_ns(\n",
    "    selected_host_timestamp_ns,\n",
    "    TimeSyncMode.SUBGHZ\n",
    ")\n",
    "print(f\"\\nConverted to client timestamp: {converted_client_timestamp_ns} ns\")\n",
    "\n",
    "# Convert back from client time to host time (roundtrip)\n",
    "roundtrip_host_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_device_time_to_synctime_ns(\n",
    "    converted_client_timestamp_ns,\n",
    "    TimeSyncMode.SUBGHZ\n",
    ")\n",
    "print(f\"\\nRoundtrip back to host timestamp: {roundtrip_host_timestamp_ns} ns\")\n",
    "\n",
    "# Calculate numerical difference\n",
    "roundtrip_error_ns = roundtrip_host_timestamp_ns - selected_host_timestamp_ns\n",
    "print(f\"\\nRoundtrip error: {roundtrip_error_ns} ns ({roundtrip_error_ns / 1e3:.3f} μs)\")\n",
    "print(\"  Note: Small numerical differences are expected due to interpolation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query APIs with TimeDomain.SUBGHZ\n",
    "\n",
    "Instead of manually converting timestamps, you can directly query client data using host timestamps by specifying `time_domain=TimeDomain.SUBGHZ`. This is more convenient when querying multiple data types.\n",
    "\n",
    "The following example shows how to query client RGB images using host timestamps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Query Client Data Using Host Timestamps\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Query host RGB image at the selected timestamp\n",
    "host_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns(\n",
    "    stream_id=rgb_stream_id,\n",
    "    time_ns=selected_host_timestamp_ns,\n",
    "    time_domain=TimeDomain.DEVICE_TIME,\n",
    "    time_query_options=TimeQueryOptions.CLOSEST\n",
    ")\n",
    "\n",
    "print(f\"\\nHost RGB frame:\")\n",
    "print(f\"  Query timestamp: {selected_host_timestamp_ns} ns\")\n",
    "print(f\"  Actual capture timestamp: {host_image_record.capture_timestamp_ns} ns\")\n",
    "print(f\"  Frame number: {host_image_record.frame_number}\")\n",
    "print(f\"  Image shape: {host_image_data.to_numpy_array().shape}\")\n",
    "\n",
    "# Query client RGB image using the SAME host timestamp, but with TimeDomain.SUBGHZ\n",
    "client_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns(\n",
    "    stream_id=rgb_stream_id,\n",
    "    time_ns=selected_host_timestamp_ns,  # Using host timestamp!\n",
    "    time_domain=TimeDomain.SUBGHZ,  # Specify SUBGHZ domain\n",
    "    time_query_options=TimeQueryOptions.CLOSEST\n",
    ")\n",
    "\n",
    "print(f\"\\nClient RGB frame (queried with host timestamp):\")\n",
    "print(f\"  Query timestamp (host domain): {selected_host_timestamp_ns} ns\")\n",
    "print(f\"  Actual capture timestamp (client device time): {client_image_record.capture_timestamp_ns} ns\")\n",
    "print(f\"  Frame number: {client_image_record.frame_number}\")\n",
    "print(f\"  Image shape: {client_image_data.to_numpy_array().shape}\")\n",
    "\n",
    "print(\"\\n✅ Successfully queried synchronized frames from both devices!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Timestamp-Aligned Trajectory and Hand Tracking Visualization\n",
    "\n",
    "Now we'll visualize timestamp-aligned data from both devices in 3D. Since MPS processes multi-device recordings together, the trajectories from both devices are already in the same world coordinate frame.\n",
    "\n",
    "### Load MPS Data\n",
    "\n",
    "First, let's load the MPS trajectories and hand tracking data from both devices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Loading MPS Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load closed loop trajectories\n",
    "host_trajectory = host_data_provider.get_mps_closed_loop_trajectory()\n",
    "client_trajectory = client_data_provider.get_mps_closed_loop_trajectory()\n",
    "\n",
    "print(f\"\\nHost trajectory: {len(host_trajectory)} poses\")\n",
    "print(f\"  Duration: {(host_trajectory[-1].tracking_timestamp - host_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds\")\n",
    "\n",
    "print(f\"\\nClient trajectory: {len(client_trajectory)} poses\")\n",
    "print(f\"  Duration: {(client_trajectory[-1].tracking_timestamp - client_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Load hand tracking results\n",
    "host_hand_tracking = host_data_provider.get_mps_hand_tracking_result_list()\n",
    "client_hand_tracking = client_data_provider.get_mps_hand_tracking_result_list()\n",
    "\n",
    "print(f\"\\nHost hand tracking: {len(host_hand_tracking)} frames\")\n",
    "print(f\"Client hand tracking: {len(client_hand_tracking)} frames\")\n",
    "\n",
    "print(\"\\n✅ MPS data loaded successfully!\")\n",
    "print(\"\\n⚠️ Note: Both trajectories are in the same world coordinate frame\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Setup\n",
    "\n",
    "We'll create a Rerun visualization with two views:\n",
    "1. **RGB View**: Synchronized RGB frames from both host and client devices\n",
    "2. **3D World View**: Trajectories and hand tracking from both devices in the shared world coordinate frame\n",
    "\n",
    "Let's define helper functions for plotting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for accumulated trajectory points\n",
    "host_trajectory_cache = []\n",
    "client_trajectory_cache = []\n",
    "\n",
    "def plot_device_pose_and_trajectory(device_label: str, pose: mps.ClosedLoopTrajectoryPose, trajectory_cache: list, color: list):\n",
    "    \"\"\"\n",
    "    Plot device pose and accumulated trajectory in 3D world view.\n",
    "\n",
    "    Args:\n",
    "        device_label: Label for the device (e.g., \"host\" or \"client\")\n",
    "        pose: ClosedLoopTrajectoryPose object\n",
    "        trajectory_cache: List to accumulate trajectory points\n",
    "        color: RGB color for trajectory line\n",
    "    \"\"\"\n",
    "    if pose is None:\n",
    "        return\n",
    "\n",
    "    # Get transform and add to trajectory cache\n",
    "    T_world_device = pose.transform_world_device\n",
    "    trajectory_cache.append(T_world_device.translation()[0])\n",
    "\n",
    "    # Plot device pose\n",
    "    rr.log(\n",
    "        f\"world/{device_label}/device\",\n",
    "        ToTransform3D(T_world_device, axis_length=0.05),\n",
    "    )\n",
    "\n",
    "    # Plot accumulated trajectory\n",
    "    if len(trajectory_cache) > 1:\n",
    "        rr.log(\n",
    "            f\"world/{device_label}/trajectory\",\n",
    "            rr.LineStrips3D(\n",
    "                [trajectory_cache],\n",
    "                colors=[color],\n",
    "                radii=0.005,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "def plot_hand_tracking(device_label: str, hand_tracking_result: mps.hand_tracking.HandTrackingResult):\n",
    "    \"\"\"\n",
    "    Plot hand tracking landmarks and skeleton in 3D world view.\n",
    "\n",
    "    Args:\n",
    "        device_label: Label for the device (e.g., \"host\" or \"client\")\n",
    "        hand_tracking_result: HandTrackingResult object\n",
    "    \"\"\"\n",
    "    # Clear previous hand tracking data\n",
    "    rr.log(f\"world/{device_label}/device/hand-tracking\", rr.Clear.recursive())\n",
    "\n",
    "    if hand_tracking_result is None:\n",
    "        return\n",
    "\n",
    "    # Plot left hand if available\n",
    "    if hand_tracking_result.left_hand is not None:\n",
    "        landmarks = hand_tracking_result.left_hand.landmark_positions_device\n",
    "        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n",
    "\n",
    "        landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS)\n",
    "        skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON)\n",
    "\n",
    "        rr.log(\n",
    "            f\"world/{device_label}/device/hand-tracking/left/landmarks\",\n",
    "            rr.Points3D(\n",
    "                positions=landmarks,\n",
    "                colors=[landmarks_style.color],\n",
    "                radii=landmarks_style.plot_3d_size,\n",
    "            ),\n",
    "        )\n",
    "        rr.log(\n",
    "            f\"world/{device_label}/device/hand-tracking/left/skeleton\",\n",
    "            rr.LineStrips3D(\n",
    "                skeleton,\n",
    "                colors=[skeleton_style.color],\n",
    "                radii=skeleton_style.plot_3d_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Plot right hand if available\n",
    "    if hand_tracking_result.right_hand is not None:\n",
    "        landmarks = hand_tracking_result.right_hand.landmark_positions_device\n",
    "        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n",
    "\n",
    "        landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS)\n",
    "        skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON)\n",
    "\n",
    "        rr.log(\n",
    "            f\"world/{device_label}/device/hand-tracking/right/landmarks\",\n",
    "            rr.Points3D(\n",
    "                positions=landmarks,\n",
    "                colors=[landmarks_style.color],\n",
    "                radii=landmarks_style.plot_3d_size,\n",
    "            ),\n",
    "        )\n",
    "        rr.log(\n",
    "            f\"world/{device_label}/device/hand-tracking/right/skeleton\",\n",
    "            rr.LineStrips3D(\n",
    "                skeleton,\n",
    "                colors=[skeleton_style.color],\n",
    "                radii=skeleton_style.plot_3d_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "def plot_rgb_image(device_label: str, image_data, image_record):\n",
    "    \"\"\"\n",
    "    Plot RGB image in the RGB view.\n",
    "\n",
    "    Args:\n",
    "        device_label: Label for the device (e.g., \"host\" or \"client\")\n",
    "        image_data: ImageData object\n",
    "        image_record: ImageDataRecord object\n",
    "    \"\"\"\n",
    "    if image_data is None:\n",
    "        return\n",
    "\n",
    "    rr.log(\n",
    "        f\"{device_label}\",\n",
    "        rr.Image(image_data.to_numpy_array())\n",
    "    )\n",
    "\n",
    "print(\"✅ Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Part 1: Timestamp-Aligned RGB Frames\n",
    "\n",
    "Let's first visualize the timestamp-aligned RGB frames from both devices side-by-side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RGB Rerun viewer\n",
    "rr.init(\"rgb_viewer\", spawn=False)\n",
    "print(\"✅ RGB viewer initialized\")\n",
    "# Get RGB stream ID\n",
    "rgb_stream_id = host_data_provider.get_vrs_stream_id_from_label(\"camera-rgb\")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_period_ns = int(2e8)  # 200ms = 5 Hz (adjust for different frame rates)\n",
    "start_offset_ns = int(3e9)     # Start 3 seconds into recording (skip initial setup)\n",
    "duration_ns = int(10e9)        # Visualize 10 seconds (adjust as needed)\n",
    "\n",
    "# Get host recording time range\n",
    "host_start_time_ns = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "host_end_time_ns = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "\n",
    "# Calculate query range\n",
    "query_start_ns = host_start_time_ns + start_offset_ns\n",
    "query_end_ns = min(query_start_ns + duration_ns, host_end_time_ns)\n",
    "\n",
    "# Clear trajectory caches for fresh visualization\n",
    "host_trajectory_cache.clear()\n",
    "client_trajectory_cache.clear()\n",
    "\n",
    "print(\"Visualization Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Host recording duration: {(host_end_time_ns - host_start_time_ns) / 1e9:.2f} seconds\")\n",
    "print(f\"Visualization start: {start_offset_ns / 1e9:.1f}s into recording\")\n",
    "print(f\"Visualization duration: {(query_end_ns - query_start_ns) / 1e9:.2f} seconds\")\n",
    "print(f\"Sampling rate: {1e9 / sampling_period_ns:.1f} Hz\")\n",
    "print(f\"Expected frames: ~{int((query_end_ns - query_start_ns) / sampling_period_ns)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"⏳ Logging RGB frames...\\n\")\n",
    "\n",
    "current_timestamp_ns = query_start_ns\n",
    "frame_count = 0\n",
    "\n",
    "while current_timestamp_ns <= query_end_ns:\n",
    "    # Query host RGB image\n",
    "    host_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns(\n",
    "        stream_id=rgb_stream_id,\n",
    "        time_ns=current_timestamp_ns,\n",
    "        time_domain=TimeDomain.DEVICE_TIME,\n",
    "        time_query_options=TimeQueryOptions.CLOSEST\n",
    "    )\n",
    "\n",
    "    # Query client RGB image using host timestamp with SUBGHZ\n",
    "    client_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns(\n",
    "        stream_id=rgb_stream_id,\n",
    "        time_ns=current_timestamp_ns,  # Host timestamp!\n",
    "        time_domain=TimeDomain.SUBGHZ,  # SUBGHZ domain for cross-device query\n",
    "        time_query_options=TimeQueryOptions.CLOSEST\n",
    "    )\n",
    "\n",
    "    # Set Rerun timestamp to the host's actual capture timestamp\n",
    "    rr.set_time_nanos(\"device_time\", host_image_record.capture_timestamp_ns)\n",
    "\n",
    "    # Plot RGB images with the correct timestamp\n",
    "    rr.log(\"rgb_image_in_host\", rr.Image(host_image_data.to_numpy_array()))\n",
    "    rr.log(\"rgb_image_in_client\", rr.Image(client_image_data.to_numpy_array()))\n",
    "\n",
    "    # Move to next timestamp\n",
    "    current_timestamp_ns += sampling_period_ns\n",
    "    frame_count += 1\n",
    "\n",
    "    # Print progress every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        print(f\"  Processed {frame_count} RGB frames...\")\n",
    "\n",
    "print(f\"\\n✅ RGB data logging complete! Processed {frame_count} frames.\")\n",
    "# Display RGB viewer\n",
    "rr.notebook_show()\n",
    "\n",
    "print(\"\\n💡 RGB Viewer - What to observe:\")\n",
    "print(\"  - Top: Host device RGB frames\")\n",
    "print(\"  - Bottom: Client device RGB frames\")\n",
    "print(\"  - Frames are synchronized using SubGHz time alignment\")\n",
    "print(\"  - May have slight timing differences (cameras not trigger-aligned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Part 2: 3D World View\n",
    "\n",
    "Now let's visualize the 3D trajectories, hand tracking, and point cloud in a shared world coordinate frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 3D World Rerun viewer\n",
    "rr.init(\"world_3d_viewer\", spawn=False)\n",
    "world_blueprint = rrb.Blueprint(\n",
    "    rrb.Spatial3DView(\n",
    "        origin=\"world\",\n",
    "        name=\"3D World View\",\n",
    "        background=[0, 0, 0],\n",
    "    ),\n",
    "    collapse_panels=True,\n",
    ")\n",
    "\n",
    "print(\"✅ 3D world viewer initialized\")\n",
    "\n",
    "# Load filtered point cloud from host\n",
    "print(\"Loading filtered point cloud from host...\")\n",
    "host_point_cloud_filtered = host_data_provider.get_mps_semidense_point_cloud_filtered(\n",
    "    filter_confidence=True,\n",
    "    max_point_count=50000  # Limit to 50k points for performance\n",
    ")\n",
    "\n",
    "# Convert to numpy array for plotting\n",
    "if host_point_cloud_filtered:\n",
    "    points_array = np.array([point.position_world for point in host_point_cloud_filtered])\n",
    "\n",
    "    # Plot point cloud as static data\n",
    "    plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD)\n",
    "    rr.log(\n",
    "        f\"world/{plot_style.label}\",\n",
    "        rr.Points3D(\n",
    "            positions=points_array,\n",
    "            colors=[plot_style.color] * len(points_array),\n",
    "            radii=plot_style.plot_3d_size,\n",
    "        ),\n",
    "        static=True,\n",
    "    )\n",
    "    print(f\"✅ Plotted {len(points_array)} filtered points from host\")\n",
    "else:\n",
    "    print(\"⚠️ No point cloud data available\")\n",
    "\n",
    "# Clear trajectory caches for fresh visualization\n",
    "host_trajectory_cache.clear()\n",
    "client_trajectory_cache.clear()\n",
    "\n",
    "print(\"⏳ Logging 3D trajectories and hand tracking...\\n\")\n",
    "\n",
    "current_timestamp_ns = query_start_ns\n",
    "frame_count = 0\n",
    "\n",
    "while current_timestamp_ns <= query_end_ns:\n",
    "    # Query host MPS pose and hand tracking\n",
    "    host_pose = host_data_provider.get_mps_interpolated_closed_loop_pose(\n",
    "        timestamp_ns=current_timestamp_ns,\n",
    "        time_domain=TimeDomain.DEVICE_TIME\n",
    "    )\n",
    "\n",
    "    host_hand = host_data_provider.get_mps_interpolated_hand_tracking_result(\n",
    "        timestamp_ns=current_timestamp_ns,\n",
    "        time_domain=TimeDomain.DEVICE_TIME\n",
    "    )\n",
    "\n",
    "    # Query client MPS pose and hand tracking using host timestamp with SUBGHZ\n",
    "    client_pose = client_data_provider.get_mps_interpolated_closed_loop_pose(\n",
    "        timestamp_ns=current_timestamp_ns,  # Host timestamp!\n",
    "        time_domain=TimeDomain.SUBGHZ  # SUBGHZ domain for cross-device query\n",
    "    )\n",
    "\n",
    "    client_hand = client_data_provider.get_mps_interpolated_hand_tracking_result(\n",
    "        timestamp_ns=current_timestamp_ns,  # Host timestamp!\n",
    "        time_domain=TimeDomain.SUBGHZ  # SUBGHZ domain for cross-device query\n",
    "    )\n",
    "\n",
    "    # Set Rerun timestamp to the host's actual pose timestamp (if available)\n",
    "    if host_pose is not None:\n",
    "        rr.set_time_nanos(\"device_time\", int(host_pose.tracking_timestamp.total_seconds() * 1e9))\n",
    "    else:\n",
    "        rr.set_time_nanos(\"device_time\", current_timestamp_ns)\n",
    "\n",
    "    # Plot host trajectory and pose (blue color) with correct timestamp\n",
    "    plot_device_pose_and_trajectory(\n",
    "        \"host\",\n",
    "        host_pose,\n",
    "        host_trajectory_cache,\n",
    "        color=[0, 100, 255]\n",
    "    )\n",
    "    plot_hand_tracking(\"host\", host_hand)\n",
    "\n",
    "    # Plot client trajectory and pose (red color) with the same timestamp\n",
    "    plot_device_pose_and_trajectory(\n",
    "        \"client\",\n",
    "        client_pose,\n",
    "        client_trajectory_cache,\n",
    "        color=[255, 100, 0]\n",
    "    )\n",
    "    plot_hand_tracking(\"client\", client_hand)\n",
    "\n",
    "    # Move to next timestamp\n",
    "    current_timestamp_ns += sampling_period_ns\n",
    "    frame_count += 1\n",
    "\n",
    "    # Print progress every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        print(f\"  Processed {frame_count} 3D frames...\")\n",
    "\n",
    "print(f\"\\n✅ 3D data logging complete! Processed {frame_count} frames.\")\n",
    "print(f\"\\n📊 Trajectory Statistics:\")\n",
    "print(f\"  Host trajectory points: {len(host_trajectory_cache)}\")\n",
    "print(f\"  Client trajectory points: {len(client_trajectory_cache)}\")\n",
    "# Display 3D world viewer\n",
    "rr.notebook_show(blueprint=world_blueprint)\n",
    "\n",
    "print(\"\\n💡 3D World Viewer - What to observe:\")\n",
    "print(\"  - Gray points: Filtered semi-dense point cloud from host\")\n",
    "print(\"  - Blue trajectory: Host device path\")\n",
    "print(\"  - Red trajectory: Client device path\")\n",
    "print(\"  - Hand skeletons: Real-time hand tracking from both devices\")\n",
    "print(\"  - Both trajectories share the same world coordinate frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial covered multi-device timestamp alignment in the Aria Gen2 Pilot Dataset:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **SubGHz timestamp alignment**: Understanding the host/client model for multi-device time alignment\n",
    "   - Host broadcasts SubGHz signals\n",
    "   - Client records time domain mapping (only in client VRS)\n",
    "   - Enables bidirectional timestamp conversion\n",
    "\n",
    "2. **Timestamp Conversion APIs**:\n",
    "   - `convert_from_synctime_to_device_time_ns()`: Host time → Client device time\n",
    "   - `convert_from_device_time_to_synctime_ns()`: Client device time → Host time\n",
    "   - Both functions called on client's VRS data provider\n",
    "\n",
    "3. **Query APIs with TimeDomain.SUBGHZ**:\n",
    "   - Query client data directly using host timestamps\n",
    "   - Specify `time_domain=TimeDomain.SUBGHZ` in query functions\n",
    "   - More convenient than manual timestamp conversion\n",
    "\n",
    "4. **Shared World Coordinate Frame**:\n",
    "   - MPS trajectories from multi-device recordings are in the same world coordinate frame\n",
    "   - Enables direct comparison and multi-view analysis\n",
    "   - No additional alignment needed\n",
    "\n",
    "5. **Visualization Best Practices**:\n",
    "   - Break large visualization code into manageable steps\n",
    "   - Use different colors for different devices\n",
    "   - Show timestamp-aligned RGB frames side-by-side\n",
    "   - Display trajectories and hand tracking in unified 3D world view\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
